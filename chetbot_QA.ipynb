{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gossiping-QA-Dataset : https://www.kaggle.com/zake7749/pttgossipingcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Gossiping-QA-Dataset-2_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>為什麼 聖結石 會被酸而 這群人 不會？</td>\n",
       "      <td>質感 劇本 成員 都差很多好嗎 不要拿腎結石來污辱這群人</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>為什麼慶祝228會被罵可是慶端午不會？</td>\n",
       "      <td>因為屈原不是台灣人，是楚國人。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>有沒有戰神阿瑞斯的八卦?</td>\n",
       "      <td>爵士就是阿瑞斯 男主角最後死了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>理論與實務最脫節的系</td>\n",
       "      <td>哪個系不脫節...你問最不脫節的簡單多了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>為什麼PTT這麼多人看棒球</td>\n",
       "      <td>肥宅才看棒球　系壘一堆胖子</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               question                        answer\n",
       "0  為什麼 聖結石 會被酸而 這群人 不會？  質感 劇本 成員 都差很多好嗎 不要拿腎結石來污辱這群人\n",
       "1   為什麼慶祝228會被罵可是慶端午不會？               因為屈原不是台灣人，是楚國人。\n",
       "2          有沒有戰神阿瑞斯的八卦?               爵士就是阿瑞斯 男主角最後死了\n",
       "3            理論與實務最脫節的系       哪個系不脫節...你問最不脫節的簡單多了...\n",
       "4         為什麼PTT這麼多人看棒球                 肥宅才看棒球　系壘一堆胖子"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774114\n",
      "774108\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df[df['question'].notnull()]\n",
    "df = df[df['answer'].notnull()]\n",
    "df = df[df['question'].notna()]\n",
    "df = df[df['answer'].notna()]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'為什麼 聖結石 會被酸而 這群人 不會？'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'質感 劇本 成員 都差很多好嗎 不要拿腎結石來污辱這群人'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\") #'bert-base-uncased'\n",
    "g_vocab_size = g_tokenizer.vocab_size\n",
    "g_max_input_length = g_tokenizer.max_model_input_sizes['bert-base-chinese']  # 512\n",
    "g_bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "g_bert_emb_dim = g_bert.config.to_dict()['hidden_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = g_tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:g_max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, train_Q, train_A):\n",
    "        \n",
    "        self.train_Q = train_Q\n",
    "        self.train_A = train_A\n",
    "        self.length = len(train_Q)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "           \n",
    "#         Q_token = tokenize_and_cut(\"[CLS] \" + str(self.train_Q[index]) + \" [SEP]\")\n",
    "#         A_token = tokenize_and_cut(\"[CLS] \" + str(self.train_A[index]) + \" [SEP]\")\n",
    "\n",
    "        Q_token = tokenize_and_cut(str(self.train_Q[index]))\n",
    "        A_token = tokenize_and_cut(str(self.train_A[index]+\"。\"))\n",
    "        \n",
    "        Q_index = g_tokenizer.convert_tokens_to_ids(Q_token)\n",
    "        A_index = g_tokenizer.convert_tokens_to_ids(A_token)\n",
    "        \n",
    "        Q_tensor = torch.tensor([Q_index]).transpose(0, 1).to(g_device)\n",
    "        A_tensor = torch.tensor([A_index]).transpose(0, 1).to(g_device)\n",
    "        \n",
    "        return Q_tensor, A_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = QADataset(df['question'], df['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4158],\n",
       "         [ 784],\n",
       "         [7938],\n",
       "         [5469],\n",
       "         [5178],\n",
       "         [4767],\n",
       "         [3298],\n",
       "         [6158],\n",
       "         [7000],\n",
       "         [5445],\n",
       "         [6857],\n",
       "         [5408],\n",
       "         [ 782],\n",
       "         [ 679],\n",
       "         [3298],\n",
       "         [8043]], device='cuda:0'),\n",
       " tensor([[6549],\n",
       "         [2697],\n",
       "         [1206],\n",
       "         [3315],\n",
       "         [2768],\n",
       "         [1519],\n",
       "         [6963],\n",
       "         [2345],\n",
       "         [2523],\n",
       "         [1914],\n",
       "         [1962],\n",
       "         [1621],\n",
       "         [ 679],\n",
       "         [6206],\n",
       "         [2897],\n",
       "         [5575],\n",
       "         [5178],\n",
       "         [4767],\n",
       "         [ 889],\n",
       "         [3738],\n",
       "         [6802],\n",
       "         [6857],\n",
       "         [5408],\n",
       "         [ 782],\n",
       "         [ 511]], device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "\n",
    "    Q_tensor = [s[0] for s in samples]\n",
    "    A_tensor = [s[1] for s in samples]\n",
    "    \n",
    "    Q_tensor = pad_sequence(Q_tensor)\n",
    "    A_tensor = pad_sequence(A_tensor)\n",
    "    \n",
    "#     masks_Q_tensors = torch.zeros(Q_tensor.size(), dtype=torch.long).to(g_device)\n",
    "#     masks_Q_tensors = masks_Q_tensors.masked_fill(Q_tensor != 0, 1)\n",
    "    \n",
    "#     masks_A_tensors = torch.zeros(A_tensor.size(), dtype=torch.long).to(g_device)\n",
    "#     masks_A_tensors = masks_A_tensors.masked_fill(A_tensor != 0, 1)\n",
    "    \n",
    "#     return Q_tensor, A_tensor, masks_Q_tensors, masks_A_tensors\n",
    "    return Q_tensor, A_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774108"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 50000\n",
    "t_set, v_set = torch.utils.data.random_split(train_set, [temp, len(df)-temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(t_set, shuffle=True, batch_size=15, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model: https://github.com/demi6od/ChatBot/blob/master/image/ChatBotBertTransformer.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransBertEncoder(nn.Module):\n",
    "    def __init__(self, nhead=8, nlayers=6, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # bert encoder\n",
    "        self.bert = g_bert\n",
    "\n",
    "        # transformer encoder, as bert last layer fine-tune\n",
    "        self.pos_encoder = PositionalEncoding(g_bert_emb_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=g_bert_emb_dim, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src len, batch size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # embedded = [src len, batch size, emb dim]\n",
    "            embedded = self.bert(src.transpose(0, 1))[0].transpose(0, 1)\n",
    "\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        src_mask = nn.Transformer().generate_square_subsequent_mask(len(embedded)).to(g_device)\n",
    "\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        outputs = self.transformer_encoder(embedded, mask = src_mask)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransBertDecoder(nn.Module):\n",
    "    def __init__(self, nhead=8, nlayers=6, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = g_bert\n",
    "\n",
    "        self.pos_decoder = PositionalEncoding(g_bert_emb_dim, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=g_bert_emb_dim, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=nlayers)\n",
    "\n",
    "        self.fc_out = nn.Linear(g_bert_emb_dim, g_vocab_size)\n",
    "\n",
    "    def forward(self, tgt, meaning, teacher_forcing_ratio = 0.01):\n",
    "        # tgt = [output_len, batch size]\n",
    "        output_len = tgt.size(0)\n",
    "        batch_size = tgt.size(1)\n",
    "        \n",
    "        # teacher_forcing_ratio = 1 -> always true\n",
    "        teacher_force = random.random() < teacher_forcing_ratio \n",
    "        \n",
    "        if teacher_force and self.training:\n",
    "            tgt_emb_total = torch.zeros(output_len, batch_size, g_bert_emb_dim).to(g_device)\n",
    "\n",
    "            for t in range(0, output_len):\n",
    "                with torch.no_grad():\n",
    "                    tgt_emb = self.bert(tgt[:t+1].transpose(0, 1))[0].transpose(0, 1)\n",
    "                        \n",
    "                tgt_emb_total[t] = tgt_emb[-1]\n",
    "\n",
    "            tgt_mask = nn.Transformer().generate_square_subsequent_mask(len(tgt_emb_total)).to(g_device)\n",
    "            decoder_output = self.transformer_decoder(tgt=tgt_emb_total,\n",
    "                                                      memory=meaning,\n",
    "                                                      tgt_mask=tgt_mask)\n",
    "            predictions = self.fc_out(decoder_output)\n",
    "        else:\n",
    "            # initialized the input of the decoder with sos_idx (start of sentence token idx)\n",
    "            output = torch.full((output_len+1, batch_size), g_tokenizer.cls_token_id, dtype=torch.long, device=g_device)\n",
    "            predictions = torch.zeros(output_len, batch_size, g_vocab_size).to(g_device)\n",
    "\n",
    "            for t in range(0, output_len):\n",
    "                with torch.no_grad():\n",
    "                    # tgt_emb = [t, batch size, emb dim]\n",
    "                    tgt_emb = self.bert(tgt[:t+1].transpose(0, 1))[0].transpose(0, 1)\n",
    "                            \n",
    "                # tgt_emb = self.pos_encoder(tgt_emb)\n",
    "                tgt_mask = nn.Transformer().generate_square_subsequent_mask(len(tgt_emb)).to(g_device)\n",
    "\n",
    "                # decoder_output = [t, batch size, emb dim]\n",
    "                decoder_output = self.transformer_decoder(tgt=tgt_emb,\n",
    "                                                          memory=meaning,\n",
    "                                                          tgt_mask=tgt_mask)\n",
    "\n",
    "                # prediction = [batch size, vocab size]\n",
    "                prediction = self.fc_out(decoder_output[-1])\n",
    "\n",
    "                # predictions = [output_len, batch size, vocab size]\n",
    "                predictions[t] = prediction\n",
    "\n",
    "                one_hot_idx = prediction.argmax(1)\n",
    "\n",
    "                # output  = [output len, batch size]\n",
    "                output[t+1] = one_hot_idx\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruEncoder(nn.Module):\n",
    "    \"\"\"compress the request embeddings to meaning\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, input_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output, hidden = self.gru(input)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(output_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, src, tgt, hidden):\n",
    "        # first input to the decoder is the <CLS> tokens\n",
    "        fc_output = src[0].unsqueeze(0)\n",
    "        tgt_len = tgt.size(0)\n",
    "        batch_size = tgt.size(1)\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(tgt_len, batch_size, g_bert_emb_dim).to(g_device)\n",
    "\n",
    "        for t in range(0, tgt_len):\n",
    "            # insert input token embedding, previous hidden state and the context state\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            gru_output, hidden = self.gru(fc_output, hidden)\n",
    "\n",
    "            fc_output = self.fc(gru_output)\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = fc_output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # ResNet, dropout on first 3 layers\n",
    "        input = self.dropout(input)\n",
    "\n",
    "        output = input + F.relu(self.fc1(input))\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        output = output + F.relu(self.fc2(output))\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        output = output + self.fc3(output)  # no relu to keep negative values\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, transbert_encoder, transbert_decoder, gru_encoder, gru_decoder, dialog_dnn):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transbert_encoder = transbert_encoder\n",
    "        self.transbert_decoder = transbert_decoder\n",
    "\n",
    "        self.gru_encoder = gru_encoder\n",
    "        self.gru_decoder = gru_decoder\n",
    "\n",
    "        self.dialog_dnn = dialog_dnn\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio):\n",
    "        request_embeddings = self.transbert_encoder(src)\n",
    "        request_meaning = self.gru_encoder(request_embeddings)\n",
    "\n",
    "        if TRAIN_DIALOG:\n",
    "            response_meaning = self.dialog_dnn(request_meaning)\n",
    "        else:\n",
    "            response_meaning = request_meaning   \n",
    "\n",
    "        response_embeddings = self.gru_decoder(request_embeddings, tgt, response_meaning)\n",
    "        response = self.transbert_decoder(tgt, response_embeddings, teacher_forcing_ratio)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chat(sentences):\n",
    "    print(\"chatbot: \", end=\"\")\n",
    "    for word_embeds in sentences:\n",
    "        #pick first\n",
    "        word_embed = word_embeds[0]\n",
    "        # find one shot index from word embedding\n",
    "        max_idx_t = word_embed.argmax()\n",
    "        max_idx = max_idx_t.item()\n",
    "        word = g_tokenizer.convert_ids_to_tokens(max_idx)\n",
    "        print(word, end=\" \")\n",
    "    print(\"\")  # new line at the end of sentence\n",
    "\n",
    "\n",
    "def print_target_tensor(sentences):\n",
    "    print(\"target: \", end=\"\")\n",
    "    for word_embeds in sentences:\n",
    "        #pick first\n",
    "        word_embed = word_embeds[0]\n",
    "        max_idx = word_embed.item()\n",
    "        word = g_tokenizer.convert_ids_to_tokens(max_idx)\n",
    "        print(word, end=\" \")\n",
    "    print(\"\")  # new line at the end of sentence\n",
    "    \n",
    "def print_input_tensor(sentences):\n",
    "    print(\"input: \", end=\"\")\n",
    "    for word_embeds in sentences:\n",
    "        #pick first\n",
    "        word_embed = word_embeds[0]\n",
    "        max_idx = word_embed.item()\n",
    "        word = g_tokenizer.convert_ids_to_tokens(max_idx)\n",
    "        print(word, end=\" \")\n",
    "    print(\"\")  # new line at the end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = g_vocab_size\n",
    "OUTPUT_DIM = g_vocab_size\n",
    "ENC_EMB_DIM = g_bert_emb_dim\n",
    "DEC_EMB_DIM = g_bert_emb_dim\n",
    "HID_DIM = 2048  # 5 * 200\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "TRANSFORMER_ENCODER_LAYER = 1  # bert fine-tune\n",
    "TRANSFORMER_DECODER_LAYER = 3  # semantics -> morphology -> syntax\n",
    "TRANSFORMER_HEAD = 8\n",
    "\n",
    "transbert_encoder = TransBertEncoder(TRANSFORMER_HEAD, TRANSFORMER_ENCODER_LAYER, ENC_DROPOUT)\n",
    "transbert_decoder = TransBertDecoder(TRANSFORMER_HEAD, TRANSFORMER_DECODER_LAYER, DEC_DROPOUT)\n",
    "gru_encoder = GruEncoder(HID_DIM, ENC_EMB_DIM)\n",
    "gru_decoder = GruDecoder(HID_DIM, DEC_EMB_DIM)\n",
    "dialog_dnn = DialogDNN(HID_DIM, HID_DIM, HID_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = Seq2Seq(transbert_encoder, transbert_decoder, gru_encoder, gru_decoder, dialog_dnn).to(g_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (transbert_encoder): TransBertEncoder(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transbert_decoder): TransBertDecoder(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (pos_decoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (transformer_decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=768, out_features=21128, bias=True)\n",
       "  )\n",
       "  (gru_encoder): GruEncoder(\n",
       "    (gru): GRU(768, 2048)\n",
       "  )\n",
       "  (gru_decoder): GruDecoder(\n",
       "    (gru): GRU(768, 2048)\n",
       "    (fc): Linear(in_features=2048, out_features=768, bias=True)\n",
       "  )\n",
       "  (dialog_dnn): DialogDNN(\n",
       "    (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (fc2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (fc3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(g_model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=g_tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774108"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3334"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = torch.load('./save/model_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (1/3334) loss : 7.070329666137695\n",
      "e : 0 (2/3334) loss : 7.196587085723877\n",
      "e : 0 (3/3334) loss : 7.10025691986084\n",
      "e : 0 (4/3334) loss : 7.145488262176514\n",
      "e : 0 (5/3334) loss : 6.854111671447754\n",
      "e : 0 (6/3334) loss : 7.39457893371582\n",
      "e : 0 (7/3334) loss : 6.831321716308594\n",
      "e : 0 (8/3334) loss : 6.961146354675293\n",
      "e : 0 (9/3334) loss : 6.7012739181518555\n",
      "e : 0 (10/3334) loss : 7.174962997436523\n",
      "input: 肥 宅 穿 雪 靴 會 讓 人 有 小 baby 的 悸 動 嗎 [PAD] [PAD] [PAD] \n",
      "target: 像 基 紐 特 戰 隊 吧 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 不 不 不 不 的 不 的 不 不 就 的 的 不 不 的 的 的 的 不 不 不 \n",
      "e : 0 (11/3334) loss : 7.284234523773193\n",
      "e : 0 (12/3334) loss : 6.500909805297852\n",
      "e : 0 (13/3334) loss : 6.9264960289001465\n",
      "e : 0 (14/3334) loss : 6.774962425231934\n",
      "e : 0 (15/3334) loss : 7.118701934814453\n",
      "e : 0 (16/3334) loss : 6.554360866546631\n",
      "e : 0 (17/3334) loss : 6.940448760986328\n",
      "e : 0 (18/3334) loss : 6.590342044830322\n",
      "e : 0 (19/3334) loss : 6.729740619659424\n",
      "e : 0 (20/3334) loss : 6.797727584838867\n",
      "input: 萬 一 謝 淑 薇 真 的 離 開 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 看 離 開 去 哪 裡 阿 去 中 國 的 話 就 是 我 們 八 卦 板 的 敵 人 了 。 \n",
      "chatbot: 不 不 就 不 的 的 不 不 不 不 的 的 的 的 不 不 的 的 不 的 的 的 的 的 \n",
      "e : 0 (21/3334) loss : 7.2305707931518555\n",
      "e : 0 (22/3334) loss : 7.308220863342285\n",
      "e : 0 (23/3334) loss : 6.819699764251709\n",
      "e : 0 (24/3334) loss : 6.9770827293396\n",
      "e : 0 (25/3334) loss : 6.747210502624512\n",
      "e : 0 (26/3334) loss : 6.842165946960449\n",
      "e : 0 (27/3334) loss : 7.056884288787842\n",
      "e : 0 (28/3334) loss : 7.009810924530029\n",
      "e : 0 (29/3334) loss : 7.036375999450684\n",
      "e : 0 (30/3334) loss : 6.772037982940674\n",
      "input: 阿 華 田 跟 美 露 有 差 嗎 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 台 灣 人 聽 到 阿 華 田 或 美 露 都 以 為 是 巧 克 力 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 不 不 不 的 不 的 的 的 的 的 的 不 的 不 的 不 的 就 的 的 的 不 \n",
      "e : 0 (31/3334) loss : 7.030227184295654\n",
      "e : 0 (32/3334) loss : 6.9442057609558105\n",
      "e : 0 (33/3334) loss : 6.780518054962158\n",
      "e : 0 (34/3334) loss : 7.0434770584106445\n",
      "e : 0 (35/3334) loss : 7.358935832977295\n",
      "e : 0 (36/3334) loss : 7.111292362213135\n",
      "e : 0 (37/3334) loss : 7.08579683303833\n",
      "e : 0 (38/3334) loss : 6.597409248352051\n",
      "e : 0 (39/3334) loss : 7.069685935974121\n",
      "e : 0 (40/3334) loss : 7.270416259765625\n",
      "input: 有 沒 有 秀 哥 的 八 卦 ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 還 不 快 跟 秀 哥 道 歉 ！ 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 的 的 一 的 不 的 不 的 不 的 不 不 的 的 的 的 不 不 的 的 的 的 \n",
      "e : 0 (41/3334) loss : 6.6280388832092285\n",
      "e : 0 (42/3334) loss : 6.465502738952637\n",
      "e : 0 (43/3334) loss : 6.9577107429504395\n",
      "e : 0 (44/3334) loss : 7.220610618591309\n",
      "e : 0 (45/3334) loss : 6.979202747344971\n",
      "e : 0 (46/3334) loss : 6.703636169433594\n",
      "e : 0 (47/3334) loss : 7.121105194091797\n",
      "e : 0 (48/3334) loss : 6.588757514953613\n",
      "e : 0 (49/3334) loss : 6.867471218109131\n",
      "e : 0 (50/3334) loss : 6.7656025886535645\n",
      "input: 這 樣 我 可 以 暴 怒 嗎 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 職 業 醫 生 名 人 大 g ##uy 是 這 樣 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 不 的 不 不 的 的 的 的 不 不 不 的 就 不 的 的 的 的 不 不 的 的 不 \n",
      "e : 0 (51/3334) loss : 7.071860313415527\n",
      "e : 0 (52/3334) loss : 6.690422058105469\n",
      "e : 0 (53/3334) loss : 6.951379299163818\n",
      "e : 0 (54/3334) loss : 6.727990627288818\n",
      "e : 0 (55/3334) loss : 6.758507251739502\n",
      "e : 0 (56/3334) loss : 7.016139984130859\n",
      "e : 0 (57/3334) loss : 6.7426581382751465\n",
      "e : 0 (58/3334) loss : 7.161837100982666\n",
      "e : 0 (59/3334) loss : 6.886380195617676\n",
      "e : 0 (60/3334) loss : 6.81849479675293\n",
      "input: 背 側 背 包 到 超 商 領 錢 都 是 什 麼 樣 的 人 ！ ？ \n",
      "target: 錢 太 多 要 用 包 包 裝 的 人 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 的 不 的 不 的 不 的 不 不 不 的 一 不 的 不 不 的 不 的 \n",
      "e : 0 (61/3334) loss : 7.219347953796387\n",
      "e : 0 (62/3334) loss : 6.943126201629639\n",
      "e : 0 (63/3334) loss : 6.915871620178223\n",
      "e : 0 (64/3334) loss : 6.619384765625\n",
      "e : 0 (65/3334) loss : 6.9482526779174805\n",
      "e : 0 (66/3334) loss : 7.087639331817627\n",
      "e : 0 (67/3334) loss : 6.763889789581299\n",
      "e : 0 (68/3334) loss : 7.0816144943237305\n",
      "e : 0 (69/3334) loss : 6.938423156738281\n",
      "e : 0 (70/3334) loss : 6.997177600860596\n",
      "input: 內 褲 的 邊 邊 開 始 跑 出 線 頭 該 怎 麼 修 補 [PAD] \n",
      "target: 舔 ， 用 口 水 黏 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 不 的 的 不 就 的 不 不 不 的 的 不 的 不 的 不 不 的 不 的 不 的 \n",
      "e : 0 (71/3334) loss : 6.788887977600098\n",
      "e : 0 (72/3334) loss : 7.1473164558410645\n",
      "e : 0 (73/3334) loss : 6.96837854385376\n",
      "e : 0 (74/3334) loss : 6.988960266113281\n",
      "e : 0 (75/3334) loss : 6.980419635772705\n",
      "e : 0 (76/3334) loss : 7.181736946105957\n",
      "e : 0 (77/3334) loss : 6.945695877075195\n",
      "e : 0 (78/3334) loss : 6.827667713165283\n",
      "e : 0 (79/3334) loss : 6.682485103607178\n",
      "e : 0 (80/3334) loss : 6.595898151397705\n",
      "input: 抖 音 在 年 輕 人 間 48 ##4 真 的 有 流 行 [UNK] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 幹 你 還 用 星 星 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 的 的 不 的 不 的 不 不 的 不 的 不 的 就 不 不 的 的 的 不 的 \n",
      "e : 0 (81/3334) loss : 6.842860221862793\n",
      "e : 0 (82/3334) loss : 6.806724548339844\n",
      "e : 0 (83/3334) loss : 7.166360378265381\n",
      "e : 0 (84/3334) loss : 7.021576881408691\n",
      "e : 0 (85/3334) loss : 6.934479236602783\n",
      "e : 0 (86/3334) loss : 7.070535182952881\n",
      "e : 0 (87/3334) loss : 6.886409759521484\n",
      "e : 0 (88/3334) loss : 7.217085361480713\n",
      "e : 0 (89/3334) loss : 6.723526477813721\n",
      "e : 0 (90/3334) loss : 6.764735221862793\n",
      "input: 有 沒 有 光 華 隊 的 八 卦 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 光 復 中 華 簡 稱 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 就 不 的 的 不 不 不 不 的 的 的 不 的 不 的 不 的 不 的 不 的 的 的 的 \n",
      "e : 0 (91/3334) loss : 7.148844242095947\n",
      "e : 0 (92/3334) loss : 7.165929317474365\n",
      "e : 0 (93/3334) loss : 6.934239864349365\n",
      "e : 0 (94/3334) loss : 6.8406548500061035\n",
      "e : 0 (95/3334) loss : 6.866079330444336\n",
      "e : 0 (96/3334) loss : 7.003268718719482\n",
      "e : 0 (97/3334) loss : 6.701202392578125\n",
      "e : 0 (98/3334) loss : 6.568800449371338\n",
      "e : 0 (99/3334) loss : 6.940755367279053\n",
      "e : 0 (100/3334) loss : 7.088630199432373\n",
      "input: 2011 年 之 後 是 不 是 沒 有 科 技 發 明 了 [PAD] [PAD] [PAD] [PAD] \n",
      "target: 11 年 到 19 年 顯 示 器 進 步 超 大 的 欸 [UNK] [UNK] 。 [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 不 不 的 不 不 不 不 不 不 不 的 不 不 的 不 的 的 \n",
      "e : 0 (101/3334) loss : 6.653716564178467\n",
      "e : 0 (102/3334) loss : 7.278027534484863\n",
      "e : 0 (103/3334) loss : 6.9804253578186035\n",
      "e : 0 (104/3334) loss : 6.579395771026611\n",
      "e : 0 (105/3334) loss : 6.8106279373168945\n",
      "e : 0 (106/3334) loss : 7.243284702301025\n",
      "e : 0 (107/3334) loss : 7.251249313354492\n",
      "e : 0 (108/3334) loss : 6.664491176605225\n",
      "e : 0 (109/3334) loss : 6.886188507080078\n",
      "e : 0 (110/3334) loss : 7.434876918792725\n",
      "input: 現 在 高 雄 風 超 大 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 不 要 欺 騙 自 己 的 心 了 風 不 大 你 內 褲 是 被 室 友 拿 去 了 。 [PAD] \n",
      "chatbot: 的 不 的 的 的 去 不 不 的 的 不 的 的 不 不 不 一 不 的 不 不 的 的 不 \n",
      "e : 0 (111/3334) loss : 6.673266410827637\n",
      "e : 0 (112/3334) loss : 6.883907794952393\n",
      "e : 0 (113/3334) loss : 6.6697187423706055\n",
      "e : 0 (114/3334) loss : 6.981412887573242\n",
      "e : 0 (115/3334) loss : 6.618719577789307\n",
      "e : 0 (116/3334) loss : 7.025731563568115\n",
      "e : 0 (117/3334) loss : 6.736183166503906\n",
      "e : 0 (118/3334) loss : 7.450746059417725\n",
      "e : 0 (119/3334) loss : 7.176685810089111\n",
      "e : 0 (120/3334) loss : 6.840151309967041\n",
      "input: 有 沒 有 粉 紅 貓 掌 的 卦 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 肉 球 擠 下 去 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 的 的 的 不 不 不 的 的 的 的 就 不 的 的 的 的 不 不 不 的 不 不 \n",
      "e : 0 (121/3334) loss : 6.573383808135986\n",
      "e : 0 (122/3334) loss : 6.768651962280273\n",
      "e : 0 (123/3334) loss : 7.4888505935668945\n",
      "e : 0 (124/3334) loss : 6.985884189605713\n",
      "e : 0 (125/3334) loss : 6.792514324188232\n",
      "e : 0 (126/3334) loss : 6.917535305023193\n",
      "e : 0 (127/3334) loss : 6.706764221191406\n",
      "e : 0 (128/3334) loss : 6.623437404632568\n",
      "e : 0 (129/3334) loss : 7.138523101806641\n",
      "e : 0 (130/3334) loss : 7.1664910316467285\n",
      "input: 0 . 25 木 可 = 1 工 作 天 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 講 中 文 好 嗎 ? 共 三 小 朋 友 啊 ? 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 的 不 不 不 的 的 不 不 的 不 不 的 的 的 的 的 不 的 不 的 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (131/3334) loss : 7.182337760925293\n",
      "e : 0 (132/3334) loss : 6.659750461578369\n",
      "e : 0 (133/3334) loss : 7.007812023162842\n",
      "e : 0 (134/3334) loss : 7.112670421600342\n",
      "e : 0 (135/3334) loss : 7.104915142059326\n",
      "e : 0 (136/3334) loss : 6.885367393493652\n",
      "e : 0 (137/3334) loss : 6.821339130401611\n",
      "e : 0 (138/3334) loss : 7.105748653411865\n",
      "e : 0 (139/3334) loss : 7.464900493621826\n",
      "e : 0 (140/3334) loss : 7.149190902709961\n",
      "input: 有 沒 有 甲 狀 腺 亢 進 的 八 卦 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 我 之 前 吃 了 兩 年 藥 也 沒 變 胖 啊 反 而 最 近 幾 年 一 直 胖 。 \n",
      "chatbot: 不 不 不 一 的 的 的 的 的 不 的 不 不 的 不 不 不 不 的 的 的 不 的 \n",
      "e : 0 (141/3334) loss : 6.978708267211914\n",
      "e : 0 (142/3334) loss : 6.688735008239746\n",
      "e : 0 (143/3334) loss : 7.106686115264893\n",
      "e : 0 (144/3334) loss : 6.850161075592041\n",
      "e : 0 (145/3334) loss : 6.814092636108398\n",
      "e : 0 (146/3334) loss : 6.666689395904541\n",
      "e : 0 (147/3334) loss : 6.942694664001465\n",
      "e : 0 (148/3334) loss : 6.962291240692139\n",
      "e : 0 (149/3334) loss : 7.2463483810424805\n",
      "e : 0 (150/3334) loss : 7.106815814971924\n",
      "input: 傑 可 會 打 敗 品 客 嗎 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 樂 事 有 點 死 鹹 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 不 不 的 的 不 的 不 不 不 的 不 的 不 不 的 的 的 的 的 的 不 \n",
      "e : 0 (151/3334) loss : 7.557004451751709\n",
      "e : 0 (152/3334) loss : 6.734438896179199\n",
      "e : 0 (153/3334) loss : 7.063568592071533\n",
      "e : 0 (154/3334) loss : 6.71021032333374\n",
      "e : 0 (155/3334) loss : 6.802988529205322\n",
      "e : 0 (156/3334) loss : 7.165205478668213\n",
      "e : 0 (157/3334) loss : 6.799352645874023\n",
      "e : 0 (158/3334) loss : 7.045628070831299\n",
      "e : 0 (159/3334) loss : 6.837889671325684\n",
      "e : 0 (160/3334) loss : 6.796982765197754\n",
      "input: 母 咪 系 列 角 色 ： 四 葉 , 千 花 只 投 1 票 選 誰 ？ \n",
      "target: 選 [UNK] 大 的 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 不 不 的 的 不 的 的 不 的 的 不 的 就 的 的 不 的 不 \n",
      "e : 0 (161/3334) loss : 6.744740009307861\n",
      "e : 0 (162/3334) loss : 6.815334320068359\n",
      "e : 0 (163/3334) loss : 6.979709625244141\n",
      "e : 0 (164/3334) loss : 7.037992477416992\n",
      "e : 0 (165/3334) loss : 7.006303310394287\n",
      "e : 0 (166/3334) loss : 6.772103309631348\n",
      "e : 0 (167/3334) loss : 6.921889305114746\n",
      "e : 0 (168/3334) loss : 7.025686740875244\n",
      "e : 0 (169/3334) loss : 6.905558109283447\n",
      "e : 0 (170/3334) loss : 7.026704788208008\n",
      "input: 台 灣 到 底 是 獨 立 好 還 不 獨 立 好 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 台 灣 一 堆 人 就 只 想 舔 中 國 [UNK] ， 還 要 談 什 麼 獨 立 。 [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 不 的 不 的 的 不 的 不 不 不 的 的 不 不 的 的 的 不 不 的 不 的 \n",
      "e : 0 (171/3334) loss : 7.040977954864502\n",
      "e : 0 (172/3334) loss : 6.775455951690674\n",
      "e : 0 (173/3334) loss : 6.780293941497803\n",
      "e : 0 (174/3334) loss : 6.881616592407227\n",
      "e : 0 (175/3334) loss : 6.735531806945801\n",
      "e : 0 (176/3334) loss : 6.916892051696777\n",
      "e : 0 (177/3334) loss : 7.159599304199219\n",
      "e : 0 (178/3334) loss : 6.969491481781006\n",
      "e : 0 (179/3334) loss : 7.017218112945557\n",
      "e : 0 (180/3334) loss : 6.975518703460693\n",
      "input: 現 在 再 回 去 讀 資 工 還 來 得 及 嗎 [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 出 國 請 選 se ， 真 心 不 騙 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 不 不 不 的 不 不 的 的 的 的 不 不 不 的 不 不 的 的 不 的 的 的 \n",
      "e : 0 (181/3334) loss : 6.8781418800354\n",
      "e : 0 (182/3334) loss : 7.1930317878723145\n",
      "e : 0 (183/3334) loss : 7.038327217102051\n",
      "e : 0 (184/3334) loss : 6.916129112243652\n",
      "e : 0 (185/3334) loss : 7.066483020782471\n",
      "e : 0 (186/3334) loss : 7.214890003204346\n",
      "e : 0 (187/3334) loss : 6.7662672996521\n",
      "e : 0 (188/3334) loss : 7.0618720054626465\n",
      "e : 0 (189/3334) loss : 6.9590349197387695\n",
      "e : 0 (190/3334) loss : 6.855737209320068\n",
      "input: 有 沒 有 第 一 間 店 要 開 什 麼 店 最 上 手 和 最 賺 ? \n",
      "target: 賭 場 是 最 賺 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 不 的 的 的 的 不 不 的 的 的 不 一 的 不 的 不 不 的 不 \n",
      "e : 0 (191/3334) loss : 7.24784517288208\n",
      "e : 0 (192/3334) loss : 6.884528160095215\n",
      "e : 0 (193/3334) loss : 7.1729559898376465\n",
      "e : 0 (194/3334) loss : 6.463944435119629\n",
      "e : 0 (195/3334) loss : 6.519932746887207\n",
      "e : 0 (196/3334) loss : 7.086531162261963\n",
      "e : 0 (197/3334) loss : 6.986084938049316\n",
      "e : 0 (198/3334) loss : 7.04919958114624\n",
      "e : 0 (199/3334) loss : 6.837246894836426\n",
      "e : 0 (200/3334) loss : 6.8924150466918945\n",
      "input: 有 沒 有 台 灣 片 金 馬 獎 參 賽 名 單 的 八 卦 ? [PAD] [PAD] [PAD] \n",
      "target: 金 馬 奬 是 報 名 制 的 ， 沒 報 名 就 不 會 列 入 比 賽 。 \n",
      "chatbot: 的 的 的 的 不 的 不 不 不 的 不 的 就 的 的 的 的 不 的 的 \n",
      "e : 0 (201/3334) loss : 7.174666881561279\n",
      "e : 0 (202/3334) loss : 6.808682441711426\n",
      "e : 0 (203/3334) loss : 7.098531246185303\n",
      "e : 0 (204/3334) loss : 6.8309006690979\n",
      "e : 0 (205/3334) loss : 6.8490519523620605\n",
      "e : 0 (206/3334) loss : 6.775423049926758\n",
      "e : 0 (207/3334) loss : 6.8542962074279785\n",
      "e : 0 (208/3334) loss : 6.670731067657471\n",
      "e : 0 (209/3334) loss : 7.0254011154174805\n",
      "e : 0 (210/3334) loss : 6.951456546783447\n",
      "input: 有 沒 有 嗯 、 喔 的 八 卦 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 老 笑 話 看 得 很 煩 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 不 不 不 的 不 的 不 不 的 的 不 的 的 不 不 不 不 不 的 的 的 \n",
      "e : 0 (211/3334) loss : 7.022770881652832\n",
      "e : 0 (212/3334) loss : 6.865093231201172\n",
      "e : 0 (213/3334) loss : 6.643867015838623\n",
      "e : 0 (214/3334) loss : 7.108799457550049\n",
      "e : 0 (215/3334) loss : 6.867760181427002\n",
      "e : 0 (216/3334) loss : 6.945483207702637\n",
      "e : 0 (217/3334) loss : 7.123847961425781\n",
      "e : 0 (218/3334) loss : 7.053876876831055\n",
      "e : 0 (219/3334) loss : 6.897681713104248\n",
      "e : 0 (220/3334) loss : 7.081748962402344\n",
      "input: 訂 不 到 火 車 票 怎 麼 辦 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 0 點 已 搶 光 2 點 沒 票 正 常 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 一 不 不 的 不 的 的 不 不 的 不 不 的 一 的 的 不 不 的 不 \n",
      "e : 0 (221/3334) loss : 6.826461315155029\n",
      "e : 0 (222/3334) loss : 6.669969081878662\n",
      "e : 0 (223/3334) loss : 6.977325439453125\n",
      "e : 0 (224/3334) loss : 6.746044635772705\n",
      "e : 0 (225/3334) loss : 7.471192836761475\n",
      "e : 0 (226/3334) loss : 6.7918243408203125\n",
      "e : 0 (227/3334) loss : 6.820558071136475\n",
      "e : 0 (228/3334) loss : 6.9532341957092285\n",
      "e : 0 (229/3334) loss : 6.8596391677856445\n",
      "e : 0 (230/3334) loss : 7.181280612945557\n",
      "input: 滿 腦 子 都 是 裸 模 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 幫 你 止 癢 啊 哈 哈 哈 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 的 的 的 不 不 的 的 不 的 的 不 不 去 的 的 一 不 不 的 的 不 的 \n",
      "e : 0 (231/3334) loss : 6.875193119049072\n",
      "e : 0 (232/3334) loss : 7.255494117736816\n",
      "e : 0 (233/3334) loss : 6.745407581329346\n",
      "e : 0 (234/3334) loss : 6.7836713790893555\n",
      "e : 0 (235/3334) loss : 6.719721794128418\n",
      "e : 0 (236/3334) loss : 7.1791558265686035\n",
      "e : 0 (237/3334) loss : 6.569018363952637\n",
      "e : 0 (238/3334) loss : 7.051311492919922\n",
      "e : 0 (239/3334) loss : 6.822977542877197\n",
      "e : 0 (240/3334) loss : 6.880176067352295\n",
      "input: 最 想 看 [UNK] 哪 個 神 人 的 真 面 目 ? ? [PAD] [PAD] [PAD] [PAD] \n",
      "target: 雞 軟 看 他 現 實 是 不 是 一 樣 幹 幹 叫 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 的 不 不 的 不 不 不 不 的 不 不 不 不 的 不 不 的 不 的 不 \n",
      "e : 0 (241/3334) loss : 7.263624668121338\n",
      "e : 0 (242/3334) loss : 6.824707984924316\n",
      "e : 0 (243/3334) loss : 7.0434699058532715\n",
      "e : 0 (244/3334) loss : 6.931107044219971\n",
      "e : 0 (245/3334) loss : 6.87008810043335\n",
      "e : 0 (246/3334) loss : 7.216060161590576\n",
      "e : 0 (247/3334) loss : 6.915958881378174\n",
      "e : 0 (248/3334) loss : 7.302860736846924\n",
      "e : 0 (249/3334) loss : 7.076915264129639\n",
      "e : 0 (250/3334) loss : 7.0640997886657715\n",
      "input: 四 川 太 伏 事 件 創 了 醫 學 新 例 嗎 ? [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 前 兩 天 五 毛 還 到 處 說 官 方 調 查 報 告 多 詳 細 呢 。 [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 不 的 的 的 不 的 不 的 的 的 的 的 的 的 不 不 不 不 的 的 不 \n",
      "e : 0 (251/3334) loss : 7.265421390533447\n",
      "e : 0 (252/3334) loss : 7.156183242797852\n",
      "e : 0 (253/3334) loss : 7.067080020904541\n",
      "e : 0 (254/3334) loss : 6.777967929840088\n",
      "e : 0 (255/3334) loss : 7.026768684387207\n",
      "e : 0 (256/3334) loss : 7.04359769821167\n",
      "e : 0 (257/3334) loss : 6.938676834106445\n",
      "e : 0 (258/3334) loss : 6.951904296875\n",
      "e : 0 (259/3334) loss : 6.582066059112549\n",
      "e : 0 (260/3334) loss : 6.985319137573242\n",
      "input: 傻 逼 這 個 詞 是 不 是 台 灣 先 用 的 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 港 片 轉 進 來 台 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 不 的 的 的 不 不 的 不 的 的 的 不 的 的 的 的 的 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (261/3334) loss : 7.128744125366211\n",
      "e : 0 (262/3334) loss : 6.922535419464111\n",
      "e : 0 (263/3334) loss : 7.1184892654418945\n",
      "e : 0 (264/3334) loss : 6.980681896209717\n",
      "e : 0 (265/3334) loss : 6.696938514709473\n",
      "e : 0 (266/3334) loss : 6.932398319244385\n",
      "e : 0 (267/3334) loss : 6.863063335418701\n",
      "e : 0 (268/3334) loss : 6.921705722808838\n",
      "e : 0 (269/3334) loss : 6.899007320404053\n",
      "e : 0 (270/3334) loss : 6.875816345214844\n",
      "input: 館 長 直 播 揍 陳 興 會 怎 樣 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 你 去 問 問 打 [UNK] 的 陳 老 師 還 有 點 機 會 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 的 不 的 的 的 不 不 一 不 的 的 的 的 的 的 不 不 的 的 不 不 的 \n",
      "e : 0 (271/3334) loss : 6.921394348144531\n",
      "e : 0 (272/3334) loss : 6.809695243835449\n",
      "e : 0 (273/3334) loss : 6.7314887046813965\n",
      "e : 0 (274/3334) loss : 7.2962212562561035\n",
      "e : 0 (275/3334) loss : 7.207239151000977\n",
      "e : 0 (276/3334) loss : 7.259853839874268\n",
      "e : 0 (277/3334) loss : 6.8211588859558105\n",
      "e : 0 (278/3334) loss : 7.023688316345215\n",
      "e : 0 (279/3334) loss : 7.078963756561279\n",
      "e : 0 (280/3334) loss : 6.528163909912109\n",
      "input: 哪 一 種 水 果 能 在 冰 箱 冷 藏 放 最 久 還 能 吃 ? [PAD] \n",
      "target: 吃 下 去 之 後 就 變 的 跟 黑 人 一 樣 又 黑 又 長 。 [PAD] [PAD] \n",
      "chatbot: 不 不 的 的 的 的 的 不 的 的 不 不 的 不 的 不 不 的 的 的 \n",
      "e : 0 (281/3334) loss : 6.717177391052246\n",
      "e : 0 (282/3334) loss : 6.958097457885742\n",
      "e : 0 (283/3334) loss : 7.175349712371826\n",
      "e : 0 (284/3334) loss : 6.966108798980713\n",
      "e : 0 (285/3334) loss : 6.751668930053711\n",
      "e : 0 (286/3334) loss : 7.3281707763671875\n",
      "e : 0 (287/3334) loss : 6.892965793609619\n",
      "e : 0 (288/3334) loss : 7.052432060241699\n",
      "e : 0 (289/3334) loss : 6.920773506164551\n",
      "e : 0 (290/3334) loss : 7.031733989715576\n",
      "input: 人 類 一 直 汙 染 海 域 水 行 俠 不 會 生 氣 嗎 [PAD] [PAD] [PAD] [PAD] \n",
      "target: 水 行 俠 他 媽 都 說 海 陸 一 家 親 了 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 的 不 的 不 不 的 不 就 的 的 不 不 的 的 的 的 不 不 的 不 \n",
      "e : 0 (291/3334) loss : 7.012956142425537\n",
      "e : 0 (292/3334) loss : 6.972255706787109\n",
      "e : 0 (293/3334) loss : 6.823939800262451\n",
      "e : 0 (294/3334) loss : 7.035578727722168\n",
      "e : 0 (295/3334) loss : 6.7846455574035645\n",
      "e : 0 (296/3334) loss : 6.994767665863037\n",
      "e : 0 (297/3334) loss : 6.702607154846191\n",
      "e : 0 (298/3334) loss : 7.383028984069824\n",
      "e : 0 (299/3334) loss : 7.018110275268555\n",
      "e : 0 (300/3334) loss : 6.910810470581055\n",
      "input: 喬 峰 是 甲 甲 還 是 雙 性 戀 ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 馬 膺 九 金 小 刀 斷 背 山 馬 英 九 金 溥 聰 斷 背 山 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 不 的 不 不 的 不 不 不 的 的 想 不 不 的 的 的 的 不 的 不 不 不 的 \n",
      "e : 0 (301/3334) loss : 6.934756755828857\n",
      "e : 0 (302/3334) loss : 6.901205539703369\n",
      "e : 0 (303/3334) loss : 7.403173446655273\n",
      "e : 0 (304/3334) loss : 6.7783942222595215\n",
      "e : 0 (305/3334) loss : 6.7728142738342285\n",
      "e : 0 (306/3334) loss : 6.895944595336914\n",
      "e : 0 (307/3334) loss : 7.093935012817383\n",
      "e : 0 (308/3334) loss : 6.892186164855957\n",
      "e : 0 (309/3334) loss : 6.804074287414551\n",
      "e : 0 (310/3334) loss : 6.893457889556885\n",
      "input: 有 沒 有 肥 宅 要 如 何 輕 鬆 減 肥 的 八 卦 [PAD] [PAD] [PAD] [PAD] \n",
      "target: 吃 肉 減 肥 法 瘦 的 快 肥 的 也 快 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 不 不 的 不 不 不 的 不 不 不 不 的 不 不 不 不 的 不 不 的 \n",
      "e : 0 (311/3334) loss : 6.5649261474609375\n",
      "e : 0 (312/3334) loss : 6.905721187591553\n",
      "e : 0 (313/3334) loss : 7.175025939941406\n",
      "e : 0 (314/3334) loss : 7.397146701812744\n",
      "e : 0 (315/3334) loss : 7.172936916351318\n",
      "e : 0 (316/3334) loss : 6.695946216583252\n",
      "e : 0 (317/3334) loss : 6.98563814163208\n",
      "e : 0 (318/3334) loss : 7.187545299530029\n",
      "e : 0 (319/3334) loss : 6.998289585113525\n",
      "e : 0 (320/3334) loss : 7.205780506134033\n",
      "input: 如 果 沒 有 發 生 林 奕 含 這 件 事 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: # 這 裡 有 很 多 憂 鬱 症 專 家 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 不 不 的 的 的 不 不 不 不 的 不 的 的 的 的 \n",
      "e : 0 (321/3334) loss : 7.095175266265869\n",
      "e : 0 (322/3334) loss : 7.065543174743652\n",
      "e : 0 (323/3334) loss : 7.006923198699951\n",
      "e : 0 (324/3334) loss : 6.969615459442139\n",
      "e : 0 (325/3334) loss : 6.971289157867432\n",
      "e : 0 (326/3334) loss : 7.047497749328613\n",
      "e : 0 (327/3334) loss : 7.057997703552246\n",
      "e : 0 (328/3334) loss : 7.018548488616943\n",
      "e : 0 (329/3334) loss : 6.98354959487915\n",
      "e : 0 (330/3334) loss : 7.173380374908447\n",
      "input: 帶 老 虎 上 救 生 艇 的 人 在 想 什 麼 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 垃 圾 爛 片 幹 0 娘 智 障 印 度 狗 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 不 的 的 的 不 不 的 的 不 不 不 的 不 的 的 不 不 的 \n",
      "e : 0 (331/3334) loss : 6.846673965454102\n",
      "e : 0 (332/3334) loss : 6.8456854820251465\n",
      "e : 0 (333/3334) loss : 7.040277004241943\n",
      "e : 0 (334/3334) loss : 6.863671779632568\n",
      "e : 0 (335/3334) loss : 6.7185893058776855\n",
      "e : 0 (336/3334) loss : 6.871222972869873\n",
      "e : 0 (337/3334) loss : 7.044222831726074\n",
      "e : 0 (338/3334) loss : 6.9553656578063965\n",
      "e : 0 (339/3334) loss : 7.109182357788086\n",
      "e : 0 (340/3334) loss : 6.678929328918457\n",
      "input: 因 為 衛 生 吃 鬍 鬚 張 的 人 平 常 怎 麼 過 活 [PAD] [PAD] [PAD] \n",
      "target: 還 沒 看 過 遊 民 以 外 的 人 覺 得 衛 生 不 重 要 的 。 \n",
      "chatbot: 的 一 的 不 不 不 的 一 的 不 不 的 不 不 的 不 不 不 的 \n",
      "e : 0 (341/3334) loss : 7.208855152130127\n",
      "e : 0 (342/3334) loss : 6.700260162353516\n",
      "e : 0 (343/3334) loss : 7.015442848205566\n",
      "e : 0 (344/3334) loss : 7.1151275634765625\n",
      "e : 0 (345/3334) loss : 6.746232032775879\n",
      "e : 0 (346/3334) loss : 7.065084934234619\n",
      "e : 0 (347/3334) loss : 6.943251132965088\n",
      "e : 0 (348/3334) loss : 6.893706321716309\n",
      "e : 0 (349/3334) loss : 6.953498363494873\n",
      "e : 0 (350/3334) loss : 6.777828216552734\n",
      "input: 用 [UNK] 的 桌 椅 高 度 應 該 配 怎 樣 才 適 合 [PAD] [PAD] \n",
      "target: 現 在 已 經 是 歐 壓 蘇 米 了 嗎 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 不 的 的 的 不 的 的 不 不 不 不 就 不 的 的 不 不 的 \n",
      "e : 0 (351/3334) loss : 6.5493974685668945\n",
      "e : 0 (352/3334) loss : 6.757279872894287\n",
      "e : 0 (353/3334) loss : 7.008389472961426\n",
      "e : 0 (354/3334) loss : 6.536497116088867\n",
      "e : 0 (355/3334) loss : 6.668942451477051\n",
      "e : 0 (356/3334) loss : 6.662939071655273\n",
      "e : 0 (357/3334) loss : 6.9179816246032715\n",
      "e : 0 (358/3334) loss : 6.671947956085205\n",
      "e : 0 (359/3334) loss : 6.827030181884766\n",
      "e : 0 (360/3334) loss : 7.075132369995117\n",
      "input: 如 果 今 天 是 孝 子 撞 上 卸 貨 的 人 孝 子 ? [PAD] [PAD] \n",
      "target: 被 撞 會 變 孝 子 就 是 台 灣 日 常 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 一 不 的 不 的 不 不 不 的 不 一 不 的 不 的 不 不 \n",
      "e : 0 (361/3334) loss : 6.913789749145508\n",
      "e : 0 (362/3334) loss : 6.823369026184082\n",
      "e : 0 (363/3334) loss : 7.010979652404785\n",
      "e : 0 (364/3334) loss : 7.0152907371521\n",
      "e : 0 (365/3334) loss : 6.813542366027832\n",
      "e : 0 (366/3334) loss : 6.914687156677246\n",
      "e : 0 (367/3334) loss : 6.60931921005249\n",
      "e : 0 (368/3334) loss : 6.8350934982299805\n",
      "e : 0 (369/3334) loss : 7.105746269226074\n",
      "e : 0 (370/3334) loss : 7.132794380187988\n",
      "input: 被 包 養 要 送 什 麼 禮 物 ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 送 跟 你 體 積 一 樣 龐 大 的 燻 雞 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 的 不 不 不 的 不 的 不 不 不 的 不 的 的 不 的 不 的 的 \n",
      "e : 0 (371/3334) loss : 6.717381477355957\n",
      "e : 0 (372/3334) loss : 7.121656894683838\n",
      "e : 0 (373/3334) loss : 6.771129131317139\n",
      "e : 0 (374/3334) loss : 6.99864387512207\n",
      "e : 0 (375/3334) loss : 6.783650875091553\n",
      "e : 0 (376/3334) loss : 6.845923900604248\n",
      "e : 0 (377/3334) loss : 6.9380950927734375\n",
      "e : 0 (378/3334) loss : 7.253973484039307\n",
      "e : 0 (379/3334) loss : 6.952560901641846\n",
      "e : 0 (380/3334) loss : 6.735446929931641\n",
      "input: 沒 有 徵 去 麥 當 勞 得 來 速 買 塑 膠 袋 的 八 卦 [PAD] \n",
      "target: 你 那 邊 還 來 的 急 趕 快 買 台 股 大 漲 100 多 點 。 [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 的 不 的 不 不 不 的 的 不 不 的 的 的 不 不 的 不 不 \n",
      "e : 0 (381/3334) loss : 6.827217102050781\n",
      "e : 0 (382/3334) loss : 6.935614585876465\n",
      "e : 0 (383/3334) loss : 7.3401079177856445\n",
      "e : 0 (384/3334) loss : 6.913602352142334\n",
      "e : 0 (385/3334) loss : 7.211526870727539\n",
      "e : 0 (386/3334) loss : 6.811038017272949\n",
      "e : 0 (387/3334) loss : 7.121032238006592\n",
      "e : 0 (388/3334) loss : 6.786139011383057\n",
      "e : 0 (389/3334) loss : 6.782370567321777\n",
      "e : 0 (390/3334) loss : 6.504115104675293\n",
      "input: 肥 胖 是 種 選 擇 ， 還 是 身 不 由 己 ？ [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 講 的 很 像 不 胖 的 人 都 不 愛 吃 很 愛 運 動 一 樣 。 [PAD] [PAD] \n",
      "chatbot: 的 不 不 的 不 不 的 的 的 不 的 不 不 的 的 的 的 不 的 的 的 \n",
      "e : 0 (391/3334) loss : 6.700733661651611\n",
      "e : 0 (392/3334) loss : 6.561152458190918\n",
      "e : 0 (393/3334) loss : 6.892497539520264\n",
      "e : 0 (394/3334) loss : 6.863165378570557\n",
      "e : 0 (395/3334) loss : 6.8153605461120605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (396/3334) loss : 7.077903747558594\n",
      "e : 0 (397/3334) loss : 6.6407904624938965\n",
      "e : 0 (398/3334) loss : 6.791876316070557\n",
      "e : 0 (399/3334) loss : 6.706681728363037\n",
      "e : 0 (400/3334) loss : 6.58101224899292\n",
      "input: 228 連 假 隔 天 大 家 會 去 上 課 嗎 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 幹 當 學 生 真 爽 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 不 的 不 就 的 不 的 的 的 不 的 不 不 的 的 的 不 的 不 不 \n",
      "e : 0 (401/3334) loss : 7.1890764236450195\n",
      "e : 0 (402/3334) loss : 6.821101188659668\n",
      "e : 0 (403/3334) loss : 7.1359710693359375\n",
      "e : 0 (404/3334) loss : 6.893045425415039\n",
      "e : 0 (405/3334) loss : 6.903397560119629\n",
      "e : 0 (406/3334) loss : 7.14539909362793\n",
      "e : 0 (407/3334) loss : 6.83892297744751\n",
      "e : 0 (408/3334) loss : 6.763853549957275\n",
      "e : 0 (409/3334) loss : 6.999514102935791\n",
      "e : 0 (410/3334) loss : 6.65871000289917\n",
      "input: 聖 誕 卡 片 接 龍 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 先 有 女 友 再 說 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 不 不 的 不 一 不 不 不 不 不 的 不 的 不 不 去 的 一 不 不 \n",
      "e : 0 (411/3334) loss : 6.91444206237793\n",
      "e : 0 (412/3334) loss : 7.199970722198486\n",
      "e : 0 (413/3334) loss : 6.943544864654541\n",
      "e : 0 (414/3334) loss : 6.712485313415527\n",
      "e : 0 (415/3334) loss : 6.788493633270264\n",
      "e : 0 (416/3334) loss : 6.691778182983398\n",
      "e : 0 (417/3334) loss : 7.008272171020508\n",
      "e : 0 (418/3334) loss : 6.747119903564453\n",
      "e : 0 (419/3334) loss : 6.809183597564697\n",
      "e : 0 (420/3334) loss : 7.023317813873291\n",
      "input: 指 甲 掀 掉 一 生 當 中 會 遇 到 幾 次 [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 上 次 也 一 樣 ， 然 後 沒 長 好 弄 到 只 好 拔 掉 2 次 。 [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 一 不 的 的 的 的 的 的 不 不 的 的 的 不 不 的 的 不 的 不 不 \n",
      "e : 0 (421/3334) loss : 6.982126712799072\n",
      "e : 0 (422/3334) loss : 6.942033767700195\n",
      "e : 0 (423/3334) loss : 6.933529853820801\n",
      "e : 0 (424/3334) loss : 7.015432834625244\n",
      "e : 0 (425/3334) loss : 6.9445013999938965\n",
      "e : 0 (426/3334) loss : 6.982990264892578\n",
      "e : 0 (427/3334) loss : 6.850717067718506\n",
      "e : 0 (428/3334) loss : 7.269543647766113\n",
      "e : 0 (429/3334) loss : 7.137176990509033\n",
      "e : 0 (430/3334) loss : 6.947836399078369\n",
      "input: 以 後 遊 行 造 勢 是 不 是 該 清 點 人 數 ? [PAD] [PAD] [PAD] \n",
      "target: 喊 一 下 部 隊 達 數 就 好 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 不 的 的 不 的 不 一 不 不 的 不 的 不 的 不 不 的 的 \n",
      "e : 0 (431/3334) loss : 6.712353706359863\n",
      "e : 0 (432/3334) loss : 7.189085960388184\n",
      "e : 0 (433/3334) loss : 7.175992965698242\n",
      "e : 0 (434/3334) loss : 6.845509052276611\n",
      "e : 0 (435/3334) loss : 6.973782539367676\n",
      "e : 0 (436/3334) loss : 7.010831832885742\n",
      "e : 0 (437/3334) loss : 7.201864719390869\n",
      "e : 0 (438/3334) loss : 7.230284214019775\n",
      "e : 0 (439/3334) loss : 7.0845866203308105\n",
      "e : 0 (440/3334) loss : 6.873998641967773\n",
      "input: 說 到 愛 你 會 先 想 到 誰 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 直 銷 愛 妳 呀 ~ 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 的 的 的 不 就 的 的 的 的 不 的 不 的 不 不 不 不 的 的 的 不 不 \n",
      "e : 0 (441/3334) loss : 6.945001602172852\n",
      "e : 0 (442/3334) loss : 6.947676181793213\n",
      "e : 0 (443/3334) loss : 6.752222061157227\n",
      "e : 0 (444/3334) loss : 7.175595760345459\n",
      "e : 0 (445/3334) loss : 6.725868225097656\n",
      "e : 0 (446/3334) loss : 6.849763870239258\n",
      "e : 0 (447/3334) loss : 7.074244022369385\n",
      "e : 0 (448/3334) loss : 7.178079128265381\n",
      "e : 0 (449/3334) loss : 6.985886096954346\n",
      "e : 0 (450/3334) loss : 7.0699615478515625\n",
      "input: 為 什 麼 不 能 把 硬 幣 完 全 用 紙 鈔 取 代 ？ [PAD] [PAD] \n",
      "target: 現 在 不 都 用 卡 片 或 行 動 支 付 了 嗎 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 不 不 的 不 不 的 的 的 不 不 的 的 的 的 的 不 的 不 不 \n",
      "e : 0 (451/3334) loss : 6.801263809204102\n",
      "e : 0 (452/3334) loss : 6.8785719871521\n",
      "e : 0 (453/3334) loss : 6.856313705444336\n",
      "e : 0 (454/3334) loss : 6.5315704345703125\n",
      "e : 0 (455/3334) loss : 6.641963481903076\n",
      "e : 0 (456/3334) loss : 7.454448223114014\n",
      "e : 0 (457/3334) loss : 6.513099670410156\n",
      "e : 0 (458/3334) loss : 6.901838302612305\n",
      "e : 0 (459/3334) loss : 6.827627182006836\n",
      "e : 0 (460/3334) loss : 6.808829307556152\n",
      "input: 停 車 繳 費 單 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 我 也 常 這 樣 其 實 不 用 繳 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 不 的 不 不 不 不 的 的 的 的 的 不 的 的 不 不 不 的 不 的 的 不 \n",
      "e : 0 (461/3334) loss : 7.258885383605957\n",
      "e : 0 (462/3334) loss : 7.100420951843262\n",
      "e : 0 (463/3334) loss : 7.253131866455078\n",
      "e : 0 (464/3334) loss : 6.884031295776367\n",
      "e : 0 (465/3334) loss : 6.859116554260254\n",
      "e : 0 (466/3334) loss : 6.6988205909729\n",
      "e : 0 (467/3334) loss : 7.041934013366699\n",
      "e : 0 (468/3334) loss : 6.866676330566406\n",
      "e : 0 (469/3334) loss : 6.8575239181518555\n",
      "e : 0 (470/3334) loss : 6.928173542022705\n",
      "input: 肥 宅 努 力 輪 班 的 意 義 是 什 麼 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 賺 錢 給 老 婆 去 菲 律 賓 被 日 本 人 幹 到 懷 孕 。 [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 一 不 的 不 不 不 不 不 不 的 的 不 的 的 的 的 不 的 不 的 \n",
      "e : 0 (471/3334) loss : 6.8990678787231445\n",
      "e : 0 (472/3334) loss : 7.240789413452148\n",
      "e : 0 (473/3334) loss : 6.782616138458252\n",
      "e : 0 (474/3334) loss : 6.91827392578125\n",
      "e : 0 (475/3334) loss : 7.66276216506958\n",
      "e : 0 (476/3334) loss : 6.957606792449951\n",
      "e : 0 (477/3334) loss : 7.072956085205078\n",
      "e : 0 (478/3334) loss : 7.0150909423828125\n",
      "e : 0 (479/3334) loss : 6.9281229972839355\n",
      "e : 0 (480/3334) loss : 7.108609676361084\n",
      "input: 誰 能 搶 救 李 明 哲 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 新 潮 流 表 示 . . . 先 顧 好 陽 信 銀 行 比 較 重 要 。 [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 不 不 的 不 的 不 的 不 的 不 的 不 的 不 不 的 不 不 的 不 的 \n",
      "e : 0 (481/3334) loss : 7.013688087463379\n",
      "e : 0 (482/3334) loss : 7.350370407104492\n",
      "e : 0 (483/3334) loss : 6.521140098571777\n",
      "e : 0 (484/3334) loss : 7.153140544891357\n",
      "e : 0 (485/3334) loss : 7.024051666259766\n",
      "e : 0 (486/3334) loss : 6.899878978729248\n",
      "e : 0 (487/3334) loss : 6.954339981079102\n",
      "e : 0 (488/3334) loss : 7.31691312789917\n",
      "e : 0 (489/3334) loss : 6.893397808074951\n",
      "e : 0 (490/3334) loss : 7.065083026885986\n",
      "input: 唱 歌 吃 什 麼 便 當 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 幹 你 媽 的 六 摟 滾 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 不 的 的 不 的 的 不 的 的 不 不 的 的 的 不 的 不 的 的 的 的 \n",
      "e : 0 (491/3334) loss : 6.903450965881348\n",
      "e : 0 (492/3334) loss : 6.93040657043457\n",
      "e : 0 (493/3334) loss : 6.96788215637207\n",
      "e : 0 (494/3334) loss : 6.780828475952148\n",
      "e : 0 (495/3334) loss : 6.88309907913208\n",
      "e : 0 (496/3334) loss : 6.796683311462402\n",
      "e : 0 (497/3334) loss : 7.105587959289551\n",
      "e : 0 (498/3334) loss : 6.722687244415283\n",
      "e : 0 (499/3334) loss : 7.077730655670166\n",
      "e : 0 (500/3334) loss : 6.926525115966797\n",
      "input: 有 沒 有 要 傳 哪 首 歌 給 喜 歡 的 女 生 的 八 卦 [PAD] [PAD] \n",
      "target: 的 是 放 棄 你 放 棄 愛 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 的 不 的 不 的 不 的 不 的 不 的 的 的 就 不 的 的 不 的 的 不 的 \n",
      "e : 0 (501/3334) loss : 6.892796516418457\n",
      "e : 0 (502/3334) loss : 6.790950298309326\n",
      "e : 0 (503/3334) loss : 6.924164295196533\n",
      "e : 0 (504/3334) loss : 6.739163398742676\n",
      "e : 0 (505/3334) loss : 7.325389385223389\n",
      "e : 0 (506/3334) loss : 7.351251125335693\n",
      "e : 0 (507/3334) loss : 6.818087577819824\n",
      "e : 0 (508/3334) loss : 7.303407192230225\n",
      "e : 0 (509/3334) loss : 6.923440933227539\n",
      "e : 0 (510/3334) loss : 7.024567604064941\n",
      "input: 讓 子 彈 飛 在 大 家 心 目 中 排 名 第 幾 啊 ？ [PAD] [PAD] [PAD] \n",
      "target: 支 那 垃 圾 電 影 一 堆 白 痴 吹 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 不 不 的 的 的 的 不 不 的 的 不 的 的 不 不 的 不 不 的 不 不 \n",
      "e : 0 (511/3334) loss : 6.824165344238281\n",
      "e : 0 (512/3334) loss : 6.989519119262695\n",
      "e : 0 (513/3334) loss : 6.8129353523254395\n",
      "e : 0 (514/3334) loss : 6.740915775299072\n",
      "e : 0 (515/3334) loss : 6.7968010902404785\n",
      "e : 0 (516/3334) loss : 6.9549150466918945\n",
      "e : 0 (517/3334) loss : 6.955169677734375\n",
      "e : 0 (518/3334) loss : 6.8790788650512695\n",
      "e : 0 (519/3334) loss : 6.75311279296875\n",
      "e : 0 (520/3334) loss : 7.176579475402832\n",
      "input: 粒 櫻 杏 子 會 想 玩 返 校 嗎 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 不 會 只 有 智 障 姆 咪 會 玩 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 的 不 不 的 不 不 的 不 不 的 不 的 的 的 的 不 不 不 的 不 的 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (521/3334) loss : 6.691389560699463\n",
      "e : 0 (522/3334) loss : 7.11459493637085\n",
      "e : 0 (523/3334) loss : 7.058228969573975\n",
      "e : 0 (524/3334) loss : 7.059263706207275\n",
      "e : 0 (525/3334) loss : 6.875002384185791\n",
      "e : 0 (526/3334) loss : 6.971358776092529\n",
      "e : 0 (527/3334) loss : 6.969923496246338\n",
      "e : 0 (528/3334) loss : 6.966388702392578\n",
      "e : 0 (529/3334) loss : 7.007637023925781\n",
      "e : 0 (530/3334) loss : 6.811379909515381\n",
      "input: 遊 行 或 是 上 街 後 面 沒 有 籌 備 可 以 辦 得 好 嗎 ? \n",
      "target: 應 該 還 是 會 有 總 召 的 收 割 這 種 事 情 不 用 擔 心 沒 人 。 [PAD] \n",
      "chatbot: 不 不 不 不 不 的 不 的 的 的 不 不 的 不 不 的 的 不 的 不 不 的 不 \n",
      "e : 0 (531/3334) loss : 6.718677520751953\n",
      "e : 0 (532/3334) loss : 6.847116947174072\n",
      "e : 0 (533/3334) loss : 6.924639701843262\n",
      "e : 0 (534/3334) loss : 6.93479061126709\n",
      "e : 0 (535/3334) loss : 6.980344295501709\n",
      "e : 0 (536/3334) loss : 6.827245712280273\n",
      "e : 0 (537/3334) loss : 7.222417831420898\n",
      "e : 0 (538/3334) loss : 6.926293849945068\n",
      "e : 0 (539/3334) loss : 6.964112281799316\n",
      "e : 0 (540/3334) loss : 6.6461181640625\n",
      "input: 為 甚 麼 9 . 2 % 都 認 為 外 國 人 不 知 道 台 灣 是 啥 ? \n",
      "target: 9 . 2 出 國 ， 要 嘛 低 調 要 嘛 說 自 己 是 台 灣 人 。 [PAD] \n",
      "chatbot: 不 不 不 不 的 的 不 不 不 的 的 的 的 不 的 就 的 的 的 不 的 \n",
      "e : 0 (541/3334) loss : 7.174492835998535\n",
      "e : 0 (542/3334) loss : 7.604515552520752\n",
      "e : 0 (543/3334) loss : 7.1326470375061035\n",
      "e : 0 (544/3334) loss : 7.062455177307129\n",
      "e : 0 (545/3334) loss : 7.088986396789551\n",
      "e : 0 (546/3334) loss : 7.042758941650391\n",
      "e : 0 (547/3334) loss : 6.701770782470703\n",
      "e : 0 (548/3334) loss : 6.7431840896606445\n",
      "e : 0 (549/3334) loss : 6.9013447761535645\n",
      "e : 0 (550/3334) loss : 6.815820693969727\n",
      "input: 有 沒 有 抗 中 奇 俠 的 八 卦 呢 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 拿 綠 卡 的 支 那 人 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 不 不 的 不 不 的 不 不 不 不 的 的 不 不 不 的 不 的 不 不 不 不 \n",
      "e : 0 (551/3334) loss : 7.355414867401123\n",
      "e : 0 (552/3334) loss : 7.057642936706543\n",
      "e : 0 (553/3334) loss : 7.099557876586914\n",
      "e : 0 (554/3334) loss : 6.9461870193481445\n",
      "e : 0 (555/3334) loss : 6.8801589012146\n",
      "e : 0 (556/3334) loss : 6.937355995178223\n",
      "e : 0 (557/3334) loss : 7.10462760925293\n",
      "e : 0 (558/3334) loss : 7.082589149475098\n",
      "e : 0 (559/3334) loss : 7.013693332672119\n",
      "e : 0 (560/3334) loss : 6.933597564697266\n",
      "input: 求 解 關 於 色 盲 ！ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 可 悲 喔 台 北 以 外 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 不 不 的 的 的 的 不 不 不 的 的 不 去 的 不 不 不 的 不 不 \n",
      "e : 0 (561/3334) loss : 6.573765277862549\n",
      "e : 0 (562/3334) loss : 6.748186111450195\n",
      "e : 0 (563/3334) loss : 6.833604335784912\n",
      "e : 0 (564/3334) loss : 7.412487030029297\n",
      "e : 0 (565/3334) loss : 6.986783504486084\n",
      "e : 0 (566/3334) loss : 6.983281135559082\n",
      "e : 0 (567/3334) loss : 7.102907180786133\n",
      "e : 0 (568/3334) loss : 6.883360385894775\n",
      "e : 0 (569/3334) loss : 6.5636162757873535\n",
      "e : 0 (570/3334) loss : 7.081713676452637\n",
      "input: 有 沒 有 [UNK] 直 沒 表 現 機 會 的 八 卦 ？ [PAD] [PAD] [PAD] [PAD] \n",
      "target: 怒 得 [UNK] 分 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 的 不 的 不 不 的 的 的 的 的 不 的 的 不 不 的 的 不 的 \n",
      "e : 0 (571/3334) loss : 6.795975685119629\n",
      "e : 0 (572/3334) loss : 7.065440654754639\n",
      "e : 0 (573/3334) loss : 6.724896430969238\n",
      "e : 0 (574/3334) loss : 7.135678291320801\n",
      "e : 0 (575/3334) loss : 6.818183422088623\n",
      "e : 0 (576/3334) loss : 7.307469844818115\n",
      "e : 0 (577/3334) loss : 6.977883338928223\n",
      "e : 0 (578/3334) loss : 6.8664021492004395\n",
      "e : 0 (579/3334) loss : 6.915098190307617\n",
      "e : 0 (580/3334) loss : 6.702888011932373\n",
      "input: 在 擁 擠 搖 晃 的 捷 運 上 補 妝 的 人 在 想 什 麼 ? [PAD] \n",
      "target: 還 有 人 坐 捷 運 時 剪 指 甲 . . . . . . . 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 不 的 的 的 不 的 不 不 的 的 不 不 的 的 不 的 不 的 的 的 不 \n",
      "e : 0 (581/3334) loss : 6.840982913970947\n",
      "e : 0 (582/3334) loss : 6.938547611236572\n",
      "e : 0 (583/3334) loss : 7.000260353088379\n",
      "e : 0 (584/3334) loss : 7.279837131500244\n",
      "e : 0 (585/3334) loss : 7.00608491897583\n",
      "e : 0 (586/3334) loss : 6.9890336990356445\n",
      "e : 0 (587/3334) loss : 7.108011722564697\n",
      "e : 0 (588/3334) loss : 6.577915191650391\n",
      "e : 0 (589/3334) loss : 7.243532657623291\n",
      "e : 0 (590/3334) loss : 6.972038269042969\n",
      "input: 怎 麼 每 台 唱 歌 都 很 可 怕 央 視 會 不 會 比 較 好 [PAD] \n",
      "target: 看 到 會 不 會 想 砸 螢 幕 阿 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 不 的 的 不 不 不 的 不 不 的 的 一 的 不 不 的 不 的 的 不 不 的 \n",
      "e : 0 (591/3334) loss : 6.739531993865967\n",
      "e : 0 (592/3334) loss : 6.689693927764893\n",
      "e : 0 (593/3334) loss : 7.012760162353516\n",
      "e : 0 (594/3334) loss : 6.964963436126709\n",
      "e : 0 (595/3334) loss : 6.941944599151611\n",
      "e : 0 (596/3334) loss : 6.690661430358887\n",
      "e : 0 (597/3334) loss : 6.8806471824646\n",
      "e : 0 (598/3334) loss : 6.983659744262695\n",
      "e : 0 (599/3334) loss : 6.906980037689209\n",
      "e : 0 (600/3334) loss : 6.699476718902588\n",
      "input: 全 世 界 最 美 的 廟 宇 是 哪 間 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 應 該 說 是 花 了 李 梅 樹 半 生 重 修 廟 不 是 他 。 [PAD] \n",
      "chatbot: 不 的 的 的 的 的 的 不 的 的 的 的 不 的 的 的 的 不 不 \n",
      "e : 0 (601/3334) loss : 6.890210151672363\n",
      "e : 0 (602/3334) loss : 7.220778942108154\n",
      "e : 0 (603/3334) loss : 6.65049409866333\n",
      "e : 0 (604/3334) loss : 7.02370023727417\n",
      "e : 0 (605/3334) loss : 7.056323528289795\n",
      "e : 0 (606/3334) loss : 6.83754825592041\n",
      "e : 0 (607/3334) loss : 7.2481560707092285\n",
      "e : 0 (608/3334) loss : 6.858836650848389\n",
      "e : 0 (609/3334) loss : 7.062667369842529\n",
      "e : 0 (610/3334) loss : 6.7329206466674805\n",
      "input: [UNK] 的 [UNK] 居 然 有 藍 勾 勾 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: ptt 有 fb 這 玩 意 ! ? 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 不 一 的 不 的 不 不 不 的 的 不 不 的 的 的 的 的 的 的 就 的 \n",
      "e : 0 (611/3334) loss : 6.797168731689453\n",
      "e : 0 (612/3334) loss : 6.653627872467041\n",
      "e : 0 (613/3334) loss : 6.6996073722839355\n",
      "e : 0 (614/3334) loss : 6.844508171081543\n",
      "e : 0 (615/3334) loss : 6.764707088470459\n",
      "e : 0 (616/3334) loss : 7.048670291900635\n",
      "e : 0 (617/3334) loss : 7.087014675140381\n",
      "e : 0 (618/3334) loss : 6.761300086975098\n",
      "e : 0 (619/3334) loss : 7.137080192565918\n",
      "e : 0 (620/3334) loss : 7.215108394622803\n",
      "input: 吃 鍋 配 什 麼 沾 醬 才 專 業 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 歡 喜 丟 好 . . 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 的 的 的 不 的 的 不 不 的 的 的 不 的 不 不 的 不 不 的 不 不 \n",
      "e : 0 (621/3334) loss : 6.837398052215576\n",
      "e : 0 (622/3334) loss : 6.9748311042785645\n",
      "e : 0 (623/3334) loss : 7.0579423904418945\n",
      "e : 0 (624/3334) loss : 6.860145568847656\n",
      "e : 0 (625/3334) loss : 7.090532302856445\n",
      "e : 0 (626/3334) loss : 7.261939525604248\n",
      "e : 0 (627/3334) loss : 6.936460018157959\n",
      "e : 0 (628/3334) loss : 6.5442118644714355\n",
      "e : 0 (629/3334) loss : 6.788917541503906\n",
      "e : 0 (630/3334) loss : 7.052655220031738\n",
      "input: 韓 國 人 會 吵 女 生 怎 不 當 兵 嗎 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 先 回 到 一 個 基 本 問 題 : 韓 國 的 網 路 論 壇 會 戰 男 女 嗎 ? 。 [PAD] \n",
      "chatbot: 的 不 的 的 不 不 不 去 不 不 的 不 的 不 的 不 的 不 的 的 不 不 不 的 不 \n",
      "e : 0 (631/3334) loss : 6.9600629806518555\n",
      "e : 0 (632/3334) loss : 7.007080078125\n",
      "e : 0 (633/3334) loss : 6.812286376953125\n",
      "e : 0 (634/3334) loss : 7.026531219482422\n",
      "e : 0 (635/3334) loss : 7.557337760925293\n",
      "e : 0 (636/3334) loss : 6.708262920379639\n",
      "e : 0 (637/3334) loss : 7.114895343780518\n",
      "e : 0 (638/3334) loss : 6.6103434562683105\n",
      "e : 0 (639/3334) loss : 6.958465576171875\n",
      "e : 0 (640/3334) loss : 6.5130085945129395\n",
      "input: 為 什 麼 拉 美 歌 曲 的 水 管 點 閱 率 都 爆 幹 高 的 啊 \n",
      "target: 西 語 也 是 美 國 第 二 大 語 言 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 的 不 不 不 不 的 的 的 不 不 一 的 的 不 的 不 不 不 的 的 \n",
      "e : 0 (641/3334) loss : 7.045832633972168\n",
      "e : 0 (642/3334) loss : 7.052815914154053\n",
      "e : 0 (643/3334) loss : 6.707973003387451\n",
      "e : 0 (644/3334) loss : 7.121259689331055\n",
      "e : 0 (645/3334) loss : 7.033831596374512\n",
      "e : 0 (646/3334) loss : 6.898262023925781\n",
      "e : 0 (647/3334) loss : 7.043457508087158\n",
      "e : 0 (648/3334) loss : 6.886575222015381\n",
      "e : 0 (649/3334) loss : 6.748124122619629\n",
      "e : 0 (650/3334) loss : 7.49869441986084\n",
      "input: 用 圖 組 成 影 片 的 故 事 有 人 相 信 ？ [PAD] [PAD] [PAD] [PAD] \n",
      "target: 通 常 常 標 題 都 是 網 路 上 瘋 傳 的 100 張 照 片 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 不 不 的 的 不 的 的 的 不 不 的 不 的 不 不 不 不 的 的 的 不 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (651/3334) loss : 7.015711784362793\n",
      "e : 0 (652/3334) loss : 6.91986608505249\n",
      "e : 0 (653/3334) loss : 6.712524890899658\n",
      "e : 0 (654/3334) loss : 6.972352504730225\n",
      "e : 0 (655/3334) loss : 6.924723148345947\n",
      "e : 0 (656/3334) loss : 7.352182865142822\n",
      "e : 0 (657/3334) loss : 7.02863073348999\n",
      "e : 0 (658/3334) loss : 6.71747350692749\n",
      "e : 0 (659/3334) loss : 6.773312568664551\n",
      "e : 0 (660/3334) loss : 6.881577014923096\n",
      "input: 怎 樣 有 禮 貌 的 欣 賞 妹 紙 的 腿 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 沒 有 圖 發 屁 文 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 不 不 的 不 不 不 的 的 不 不 的 不 的 的 \n",
      "e : 0 (661/3334) loss : 6.950395107269287\n",
      "e : 0 (662/3334) loss : 6.753802299499512\n",
      "e : 0 (663/3334) loss : 6.856932640075684\n",
      "e : 0 (664/3334) loss : 7.209568023681641\n",
      "e : 0 (665/3334) loss : 6.638875484466553\n",
      "e : 0 (666/3334) loss : 6.930627346038818\n",
      "e : 0 (667/3334) loss : 6.957139015197754\n",
      "e : 0 (668/3334) loss : 6.830349922180176\n",
      "e : 0 (669/3334) loss : 6.973723888397217\n",
      "e : 0 (670/3334) loss : 7.026264667510986\n",
      "input: 有 沒 有 寄 生 蟲 的 八 卦 ? ? ? [PAD] [PAD] [PAD] [PAD] \n",
      "target: 一 看 就 知 道 是 在 說 猛 毒 了 . . . 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 的 就 不 不 的 的 不 不 的 不 的 不 不 的 的 不 的 的 的 的 \n",
      "e : 0 (671/3334) loss : 6.78298807144165\n",
      "e : 0 (672/3334) loss : 7.162991523742676\n",
      "e : 0 (673/3334) loss : 6.7118682861328125\n",
      "e : 0 (674/3334) loss : 6.632951736450195\n",
      "e : 0 (675/3334) loss : 6.926982879638672\n",
      "e : 0 (676/3334) loss : 6.900092601776123\n",
      "e : 0 (677/3334) loss : 7.058676242828369\n",
      "e : 0 (678/3334) loss : 6.808051109313965\n",
      "e : 0 (679/3334) loss : 7.134831428527832\n",
      "e : 0 (680/3334) loss : 7.0225677490234375\n",
      "input: 老 闆 說 沒 錢 發 年 終 結 果 帶 馬 子 去 坐 遊 輪 ？ \n",
      "target: 虧 錢 時 把 營 利 損 失 轉 嫁 員 工 ， 賺 錢 時 把 盈 餘 暗 槓 。 。 \n",
      "chatbot: 不 不 不 不 不 的 的 的 不 不 不 的 的 不 不 的 不 的 的 的 不 的 不 \n",
      "e : 0 (681/3334) loss : 6.902815341949463\n",
      "e : 0 (682/3334) loss : 6.778378486633301\n",
      "e : 0 (683/3334) loss : 7.066329479217529\n",
      "e : 0 (684/3334) loss : 6.925966739654541\n",
      "e : 0 (685/3334) loss : 6.806465148925781\n",
      "e : 0 (686/3334) loss : 7.146860599517822\n",
      "e : 0 (687/3334) loss : 7.14379358291626\n",
      "e : 0 (688/3334) loss : 7.28459358215332\n",
      "e : 0 (689/3334) loss : 7.06935977935791\n",
      "e : 0 (690/3334) loss : 7.0476603507995605\n",
      "input: 台 鐵 要 怎 麼 改 革 才 有 救 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 救 了 一 個 台 鐵 於 事 無 補 這 是 通 病 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 的 不 不 的 不 不 不 就 不 不 不 不 的 的 不 不 的 的 的 \n",
      "e : 0 (691/3334) loss : 6.9554123878479\n",
      "e : 0 (692/3334) loss : 7.187417030334473\n",
      "e : 0 (693/3334) loss : 6.64772367477417\n",
      "e : 0 (694/3334) loss : 7.160129547119141\n",
      "e : 0 (695/3334) loss : 7.037604331970215\n",
      "e : 0 (696/3334) loss : 6.9695820808410645\n",
      "e : 0 (697/3334) loss : 6.8508148193359375\n",
      "e : 0 (698/3334) loss : 6.945835590362549\n",
      "e : 0 (699/3334) loss : 6.977344036102295\n",
      "e : 0 (700/3334) loss : 7.419020175933838\n",
      "input: 有 沒 有 補 噓 的 八 卦 ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 幹 中 邪 了 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 的 一 的 不 的 不 的 不 就 不 不 的 不 不 的 不 不 的 不 \n",
      "e : 0 (701/3334) loss : 6.808530330657959\n",
      "e : 0 (702/3334) loss : 6.975696563720703\n",
      "e : 0 (703/3334) loss : 7.251239776611328\n",
      "e : 0 (704/3334) loss : 6.997431755065918\n",
      "e : 0 (705/3334) loss : 7.045289993286133\n",
      "e : 0 (706/3334) loss : 6.710826396942139\n",
      "e : 0 (707/3334) loss : 7.000277519226074\n",
      "e : 0 (708/3334) loss : 7.07070255279541\n",
      "e : 0 (709/3334) loss : 7.072482585906982\n",
      "e : 0 (710/3334) loss : 6.959497928619385\n",
      "input: 推 薦 一 本 天 文 學 的 書 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 這 篇 的 重 點 是 想 哭 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 不 不 的 不 的 不 的 不 不 的 的 的 不 的 不 不 不 \n",
      "e : 0 (711/3334) loss : 6.871132850646973\n",
      "e : 0 (712/3334) loss : 7.073576927185059\n",
      "e : 0 (713/3334) loss : 7.019403457641602\n",
      "e : 0 (714/3334) loss : 7.071545600891113\n",
      "e : 0 (715/3334) loss : 7.090761661529541\n",
      "e : 0 (716/3334) loss : 7.0764665603637695\n",
      "e : 0 (717/3334) loss : 6.93848991394043\n",
      "e : 0 (718/3334) loss : 6.925591945648193\n",
      "e : 0 (719/3334) loss : 6.803974628448486\n",
      "e : 0 (720/3334) loss : 6.95023775100708\n",
      "input: 我 媽 死 了 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 嗆 他 支 那 人 死 全 家 腦 殘 支 那 人 幹 這 樣 。 [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 不 不 不 的 不 的 的 不 的 不 的 的 的 的 一 不 不 不 \n",
      "e : 0 (721/3334) loss : 6.834206581115723\n",
      "e : 0 (722/3334) loss : 7.10429573059082\n",
      "e : 0 (723/3334) loss : 6.9279680252075195\n",
      "e : 0 (724/3334) loss : 6.927736759185791\n",
      "e : 0 (725/3334) loss : 6.6771345138549805\n",
      "e : 0 (726/3334) loss : 7.119060516357422\n",
      "e : 0 (727/3334) loss : 7.091212272644043\n",
      "e : 0 (728/3334) loss : 6.860287189483643\n",
      "e : 0 (729/3334) loss : 7.003871440887451\n",
      "e : 0 (730/3334) loss : 6.65831184387207\n",
      "input: 發 現 同 事 在 看 [UNK] 片 囧 摸 辦 ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 問 他 番 號 阿 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 的 的 不 不 不 不 不 不 不 不 不 不 的 不 不 不 的 不 的 的 \n",
      "e : 0 (731/3334) loss : 7.0562005043029785\n",
      "e : 0 (732/3334) loss : 6.8634233474731445\n",
      "e : 0 (733/3334) loss : 7.086051940917969\n",
      "e : 0 (734/3334) loss : 7.184654235839844\n",
      "e : 0 (735/3334) loss : 7.13400411605835\n",
      "e : 0 (736/3334) loss : 6.6946702003479\n",
      "e : 0 (737/3334) loss : 6.9697184562683105\n",
      "e : 0 (738/3334) loss : 7.022489547729492\n",
      "e : 0 (739/3334) loss : 6.990362167358398\n",
      "e : 0 (740/3334) loss : 7.075733184814453\n",
      "input: 撿 到 鳥 該 如 何 處 置 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 臉 書 找 \" 台 灣 野 生 鳥 類 緊 急 救 助 平 台 \" 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 不 不 不 的 不 不 不 不 不 的 不 不 的 不 的 不 的 不 的 不 \n",
      "e : 0 (741/3334) loss : 7.012309551239014\n",
      "e : 0 (742/3334) loss : 6.740192890167236\n",
      "e : 0 (743/3334) loss : 7.2058539390563965\n",
      "e : 0 (744/3334) loss : 6.939670562744141\n",
      "e : 0 (745/3334) loss : 6.806514739990234\n",
      "e : 0 (746/3334) loss : 6.712797164916992\n",
      "e : 0 (747/3334) loss : 6.769890308380127\n",
      "e : 0 (748/3334) loss : 6.908636093139648\n",
      "e : 0 (749/3334) loss : 6.926904201507568\n",
      "e : 0 (750/3334) loss : 6.711556434631348\n",
      "input: 相 親 約 高 級 烤 懶 叫 店 ， 有 好 感 嗎 ^ [UNK] ^ [PAD] \n",
      "target: 男 性 去 保 證 有 鳥 無 歸 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 的 一 不 的 的 的 的 不 不 不 的 不 的 不 不 不 不 的 \n",
      "e : 0 (751/3334) loss : 7.082768440246582\n",
      "e : 0 (752/3334) loss : 6.901846408843994\n",
      "e : 0 (753/3334) loss : 7.198927879333496\n",
      "e : 0 (754/3334) loss : 7.036627769470215\n",
      "e : 0 (755/3334) loss : 7.400911331176758\n",
      "e : 0 (756/3334) loss : 6.929884433746338\n",
      "e : 0 (757/3334) loss : 7.087167263031006\n",
      "e : 0 (758/3334) loss : 6.746337413787842\n",
      "e : 0 (759/3334) loss : 7.080090045928955\n",
      "e : 0 (760/3334) loss : 6.58588171005249\n",
      "input: 千 千 進 食 中 vs 木 下 佑 香 哪 個 強 ？ [PAD] [PAD] \n",
      "target: 千 千 有 參 加 大 胃 王 比 賽 ， 看 也 知 道 完 全 比 不 上 木 下 。 [PAD] \n",
      "chatbot: 的 的 不 的 的 的 不 一 不 不 不 的 不 的 不 不 的 不 不 的 的 的 不 的 \n",
      "e : 0 (761/3334) loss : 7.116012096405029\n",
      "e : 0 (762/3334) loss : 7.099052906036377\n",
      "e : 0 (763/3334) loss : 7.266194820404053\n",
      "e : 0 (764/3334) loss : 6.927578926086426\n",
      "e : 0 (765/3334) loss : 6.814116477966309\n",
      "e : 0 (766/3334) loss : 6.977180004119873\n",
      "e : 0 (767/3334) loss : 7.035677909851074\n",
      "e : 0 (768/3334) loss : 6.8913750648498535\n",
      "e : 0 (769/3334) loss : 7.020806312561035\n",
      "e : 0 (770/3334) loss : 6.816876411437988\n",
      "input: 馬 祖 擋 不 下 會 變 歷 史 罪 人 嗎 ？ [PAD] [PAD] [PAD] [PAD] \n",
      "target: 金 馬 分 不 清 喔 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 的 的 的 不 不 不 不 的 的 的 的 不 不 不 的 不 一 不 的 的 不 \n",
      "e : 0 (771/3334) loss : 7.057394981384277\n",
      "e : 0 (772/3334) loss : 6.891237258911133\n",
      "e : 0 (773/3334) loss : 6.900911808013916\n",
      "e : 0 (774/3334) loss : 6.495982646942139\n",
      "e : 0 (775/3334) loss : 7.309617042541504\n",
      "e : 0 (776/3334) loss : 7.226354598999023\n",
      "e : 0 (777/3334) loss : 6.650755405426025\n",
      "e : 0 (778/3334) loss : 6.784493446350098\n",
      "e : 0 (779/3334) loss : 6.732013702392578\n",
      "e : 0 (780/3334) loss : 6.835901737213135\n",
      "input: 為 什 麼 第 一 節 課 的 時 間 是 早 上 8 點 ? [PAD] [PAD] [PAD] [PAD] \n",
      "target: 我 都 選 足 球 課 不 知 道 為 啥 都 沒 人 選 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 不 不 不 的 的 一 的 的 不 的 不 的 的 就 的 的 不 的 的 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (781/3334) loss : 7.003927230834961\n",
      "e : 0 (782/3334) loss : 6.8980326652526855\n",
      "e : 0 (783/3334) loss : 6.914371490478516\n",
      "e : 0 (784/3334) loss : 7.064876079559326\n",
      "e : 0 (785/3334) loss : 6.8221821784973145\n",
      "e : 0 (786/3334) loss : 7.019160747528076\n",
      "e : 0 (787/3334) loss : 7.331934928894043\n",
      "e : 0 (788/3334) loss : 7.164636135101318\n",
      "e : 0 (789/3334) loss : 6.945629596710205\n",
      "e : 0 (790/3334) loss : 7.055440425872803\n",
      "input: 鄉 民 要 怎 麼 幫 助 騎 士 贏 球 ？ [PAD] [PAD] [PAD] [PAD] \n",
      "target: 我 壓 延 長 賽 . . . 74 快 call 乾 爹 護 駕 阿 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 的 不 不 的 不 不 的 的 的 的 的 的 不 的 不 的 不 的 的 的 \n",
      "e : 0 (791/3334) loss : 7.362661838531494\n",
      "e : 0 (792/3334) loss : 7.099806785583496\n",
      "e : 0 (793/3334) loss : 6.959193706512451\n",
      "e : 0 (794/3334) loss : 6.880521774291992\n",
      "e : 0 (795/3334) loss : 7.057114124298096\n",
      "e : 0 (796/3334) loss : 7.0933990478515625\n",
      "e : 0 (797/3334) loss : 6.772912979125977\n",
      "e : 0 (798/3334) loss : 7.033262252807617\n",
      "e : 0 (799/3334) loss : 6.859582424163818\n",
      "e : 0 (800/3334) loss : 7.118099689483643\n",
      "input: 輕 軌 是 高 雄 人 的 驕 傲 嗎 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 我 覺 得 很 好 阿 ， 去 高 雄 找 朋 友 坐 過 一 次 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 一 不 的 不 不 不 不 的 不 的 的 不 的 不 不 的 不 不 的 不 不 的 的 \n",
      "e : 0 (801/3334) loss : 6.893693447113037\n",
      "e : 0 (802/3334) loss : 7.386640548706055\n",
      "e : 0 (803/3334) loss : 7.235121250152588\n",
      "e : 0 (804/3334) loss : 6.878848552703857\n",
      "e : 0 (805/3334) loss : 6.874845504760742\n",
      "e : 0 (806/3334) loss : 7.2857441902160645\n",
      "e : 0 (807/3334) loss : 6.716580390930176\n",
      "e : 0 (808/3334) loss : 6.945825099945068\n",
      "e : 0 (809/3334) loss : 6.899718284606934\n",
      "e : 0 (810/3334) loss : 6.960933208465576\n",
      "input: 如 果 把 庫 克 、 彥 州 、 四 叉 貓 放 在 荒 島 一 個 月 [PAD] \n",
      "target: 1 和 2 等 級 差 這 麼 多 懶 覺 比 雞 腿 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 不 的 不 的 不 的 不 的 不 的 的 不 不 不 的 的 的 不 \n",
      "e : 0 (811/3334) loss : 6.995706558227539\n",
      "e : 0 (812/3334) loss : 6.672674655914307\n",
      "e : 0 (813/3334) loss : 6.736941814422607\n",
      "e : 0 (814/3334) loss : 6.884552955627441\n",
      "e : 0 (815/3334) loss : 7.02370023727417\n",
      "e : 0 (816/3334) loss : 7.29555606842041\n",
      "e : 0 (817/3334) loss : 6.975310802459717\n",
      "e : 0 (818/3334) loss : 6.82249116897583\n",
      "e : 0 (819/3334) loss : 6.835913181304932\n",
      "e : 0 (820/3334) loss : 7.018042087554932\n",
      "input: 為 何 中 南 部 人 這 麼 喜 歡 住 透 天 ? [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 一 兩 個 人 住 大 樓 是 還 好 再 多 點 人 口 就 真 的 要 換 透 天 了 。 \n",
      "chatbot: 不 不 的 的 不 的 不 的 的 的 不 不 不 的 不 不 不 的 的 不 不 的 不 不 \n",
      "e : 0 (821/3334) loss : 7.0747151374816895\n",
      "e : 0 (822/3334) loss : 7.074361324310303\n",
      "e : 0 (823/3334) loss : 6.646507263183594\n",
      "e : 0 (824/3334) loss : 6.802529811859131\n",
      "e : 0 (825/3334) loss : 7.189095973968506\n",
      "e : 0 (826/3334) loss : 6.8647871017456055\n",
      "e : 0 (827/3334) loss : 6.747486591339111\n",
      "e : 0 (828/3334) loss : 6.949092864990234\n",
      "e : 0 (829/3334) loss : 7.207714557647705\n",
      "e : 0 (830/3334) loss : 7.024129390716553\n",
      "input: 妹 妹 被 肉 搜 的 八 卦 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 手 槍 ， 打 一 打 ， 打 完 ， 快 去 睡 。 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 不 的 不 的 的 的 的 不 的 的 不 的 一 的 不 的 不 \n",
      "e : 0 (831/3334) loss : 6.886822700500488\n",
      "e : 0 (832/3334) loss : 7.164578437805176\n",
      "e : 0 (833/3334) loss : 7.006906986236572\n",
      "e : 0 (834/3334) loss : 6.8683905601501465\n",
      "e : 0 (835/3334) loss : 6.905015468597412\n",
      "e : 0 (836/3334) loss : 6.819752216339111\n",
      "e : 0 (837/3334) loss : 6.76741886138916\n",
      "e : 0 (838/3334) loss : 6.738522052764893\n",
      "e : 0 (839/3334) loss : 6.973278522491455\n",
      "e : 0 (840/3334) loss : 6.712672710418701\n",
      "input: 這 個 版 有 愛 情 騙 子 嗎 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 人 妻 ， 刺 激 約 跑 那 位 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 不 的 的 的 的 一 不 的 的 的 的 的 的 的 的 不 不 不 的 不 \n",
      "e : 0 (841/3334) loss : 6.811785697937012\n",
      "e : 0 (842/3334) loss : 6.838416576385498\n",
      "e : 0 (843/3334) loss : 6.899412155151367\n",
      "e : 0 (844/3334) loss : 7.008629322052002\n",
      "e : 0 (845/3334) loss : 7.202751636505127\n",
      "e : 0 (846/3334) loss : 6.583052635192871\n",
      "e : 0 (847/3334) loss : 6.958886623382568\n",
      "e : 0 (848/3334) loss : 6.90217399597168\n",
      "e : 0 (849/3334) loss : 7.081882953643799\n",
      "e : 0 (850/3334) loss : 6.650504112243652\n",
      "input: 為 什 麼 近 16 年 日 本 不 會 拍 真 人 電 影 ? [PAD] [PAD] [PAD] [PAD] \n",
      "target: 台 灣 人 也 拍 不 出 來 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 不 不 不 不 的 不 的 不 一 的 不 的 的 不 不 不 不 的 的 不 的 的 \n",
      "e : 0 (851/3334) loss : 7.086019992828369\n",
      "e : 0 (852/3334) loss : 7.215409278869629\n",
      "e : 0 (853/3334) loss : 7.16400671005249\n",
      "e : 0 (854/3334) loss : 6.997827053070068\n",
      "e : 0 (855/3334) loss : 7.228054046630859\n",
      "e : 0 (856/3334) loss : 7.058753967285156\n",
      "e : 0 (857/3334) loss : 6.732804298400879\n",
      "e : 0 (858/3334) loss : 6.990560054779053\n",
      "e : 0 (859/3334) loss : 7.079395294189453\n",
      "e : 0 (860/3334) loss : 6.744882106781006\n",
      "input: 如 果 在 其 他 國 家 拒 絕 攔 查 會 怎 樣 [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 在 美 國 拒 絕 盤 查 直 接 開 槍 轟 爆 你 的 腦 袋 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 的 的 的 不 的 的 的 不 不 不 不 的 的 不 的 的 的 的 不 不 的 \n",
      "e : 0 (861/3334) loss : 7.025169849395752\n",
      "e : 0 (862/3334) loss : 6.8958024978637695\n",
      "e : 0 (863/3334) loss : 6.8454155921936035\n",
      "e : 0 (864/3334) loss : 6.937080383300781\n",
      "e : 0 (865/3334) loss : 6.935533046722412\n",
      "e : 0 (866/3334) loss : 6.9780778884887695\n",
      "e : 0 (867/3334) loss : 6.930706024169922\n",
      "e : 0 (868/3334) loss : 6.707900524139404\n",
      "e : 0 (869/3334) loss : 6.69632625579834\n",
      "e : 0 (870/3334) loss : 6.796066761016846\n",
      "input: 肥 宅 去 夜 市 ， 必 買 什 麼 食 物 ？ [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 肥 豬 肥 豬 夜 裡 哭 哭 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 不 不 的 的 的 不 的 不 不 的 不 的 不 不 的 不 不 的 的 的 不 \n",
      "e : 0 (871/3334) loss : 6.730125904083252\n",
      "e : 0 (872/3334) loss : 6.849913120269775\n",
      "e : 0 (873/3334) loss : 6.798604965209961\n",
      "e : 0 (874/3334) loss : 6.801062107086182\n",
      "e : 0 (875/3334) loss : 7.137697219848633\n",
      "e : 0 (876/3334) loss : 6.824416637420654\n",
      "e : 0 (877/3334) loss : 7.098731517791748\n",
      "e : 0 (878/3334) loss : 6.731410026550293\n",
      "e : 0 (879/3334) loss : 6.963125228881836\n",
      "e : 0 (880/3334) loss : 6.584764003753662\n",
      "input: 房 仲 、 直 銷 、 保 險 那 一 個 行 業 比 較 有 前 景 ？ \n",
      "target: 保 險 很 重 要 家 人 生 病 還 好 有 保 醫 療 險 。 [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 一 不 的 不 的 不 的 的 的 不 的 不 不 不 不 不 不 的 不 的 不 \n",
      "e : 0 (881/3334) loss : 7.245317459106445\n",
      "e : 0 (882/3334) loss : 6.882908821105957\n",
      "e : 0 (883/3334) loss : 7.104499816894531\n",
      "e : 0 (884/3334) loss : 7.0344743728637695\n",
      "e : 0 (885/3334) loss : 7.261490821838379\n",
      "e : 0 (886/3334) loss : 7.07282829284668\n",
      "e : 0 (887/3334) loss : 6.878334999084473\n",
      "e : 0 (888/3334) loss : 6.925965785980225\n",
      "e : 0 (889/3334) loss : 6.961937427520752\n",
      "e : 0 (890/3334) loss : 6.925243854522705\n",
      "input: 查 克 羅 禮 士 去 台 中 可 以 撐 多 久 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 羅 禮 士 到 了 台 中 ， 跺 了 腳 ， 於 是 出 現 了 台 。 [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 的 的 的 一 的 不 的 不 的 不 不 的 的 不 的 的 的 不 不 不 不 \n",
      "e : 0 (891/3334) loss : 7.078016757965088\n",
      "e : 0 (892/3334) loss : 6.893186092376709\n",
      "e : 0 (893/3334) loss : 6.880894660949707\n",
      "e : 0 (894/3334) loss : 6.889150142669678\n",
      "e : 0 (895/3334) loss : 6.953019618988037\n",
      "e : 0 (896/3334) loss : 7.002342700958252\n",
      "e : 0 (897/3334) loss : 7.316705226898193\n",
      "e : 0 (898/3334) loss : 6.8892412185668945\n",
      "e : 0 (899/3334) loss : 7.096261501312256\n",
      "e : 0 (900/3334) loss : 7.102581024169922\n",
      "input: 我 是 阿 肥 我 有 話 要 說 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 幹 您 媽 錯 字 一 堆 操 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 不 不 不 的 的 的 的 的 不 就 的 的 的 不 不 不 不 不 的 不 不 \n",
      "e : 0 (901/3334) loss : 7.06332540512085\n",
      "e : 0 (902/3334) loss : 6.585836410522461\n",
      "e : 0 (903/3334) loss : 7.060095310211182\n",
      "e : 0 (904/3334) loss : 6.889282703399658\n",
      "e : 0 (905/3334) loss : 6.656489372253418\n",
      "e : 0 (906/3334) loss : 6.864412307739258\n",
      "e : 0 (907/3334) loss : 7.126534938812256\n",
      "e : 0 (908/3334) loss : 6.972258567810059\n",
      "e : 0 (909/3334) loss : 6.77783203125\n",
      "e : 0 (910/3334) loss : 7.090076923370361\n",
      "input: 在 台 中 用 遠 光 燈 閃 一 下 會 吃 慶 記 嗎 [PAD] \n",
      "target: 你 可 以 先 試 試 看 再 上 奶 報 告 心 得 ㄛ 。 [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 的 的 的 的 不 不 不 的 的 不 一 不 的 一 的 不 一 的 \n",
      "e : 0 (911/3334) loss : 7.456201553344727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (912/3334) loss : 6.984925270080566\n",
      "e : 0 (913/3334) loss : 7.092807292938232\n",
      "e : 0 (914/3334) loss : 6.923015117645264\n",
      "e : 0 (915/3334) loss : 7.028698921203613\n",
      "e : 0 (916/3334) loss : 7.005003452301025\n",
      "e : 0 (917/3334) loss : 7.031404495239258\n",
      "e : 0 (918/3334) loss : 6.701136112213135\n",
      "e : 0 (919/3334) loss : 7.129680633544922\n",
      "e : 0 (920/3334) loss : 6.883603096008301\n",
      "input: 吃 屎 哥 是 不 是 賺 到 一 千 萬 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 就 一 堆 人 想 免 費 蹭 熱 度 阿 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 的 不 的 的 的 的 不 不 的 的 的 不 的 不 不 不 的 不 不 的 \n",
      "e : 0 (921/3334) loss : 6.997321605682373\n",
      "e : 0 (922/3334) loss : 6.78893518447876\n",
      "e : 0 (923/3334) loss : 7.085141658782959\n",
      "e : 0 (924/3334) loss : 6.652520656585693\n",
      "e : 0 (925/3334) loss : 7.102802276611328\n",
      "e : 0 (926/3334) loss : 6.801201820373535\n",
      "e : 0 (927/3334) loss : 6.679989337921143\n",
      "e : 0 (928/3334) loss : 6.89788818359375\n",
      "e : 0 (929/3334) loss : 7.140954971313477\n",
      "e : 0 (930/3334) loss : 6.882613658905029\n",
      "input: 人 馬 大 便 有 辦 法 擦 屁 股 嗎 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 用 舔 的 好 嗎 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 不 的 不 的 的 的 不 的 不 的 不 不 不 的 的 不 的 的 的 的 不 的 \n",
      "e : 0 (931/3334) loss : 6.950634479522705\n",
      "e : 0 (932/3334) loss : 7.525252819061279\n",
      "e : 0 (933/3334) loss : 6.792455196380615\n",
      "e : 0 (934/3334) loss : 6.790011405944824\n",
      "e : 0 (935/3334) loss : 7.116481304168701\n",
      "e : 0 (936/3334) loss : 7.379974842071533\n",
      "e : 0 (937/3334) loss : 7.024994373321533\n",
      "e : 0 (938/3334) loss : 6.939690113067627\n",
      "e : 0 (939/3334) loss : 7.165258407592773\n",
      "e : 0 (940/3334) loss : 6.829105854034424\n",
      "input: 想 找 一 位 穿 草 莓 內 褲 的 黑 長 髮 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 幹 我 她 媽 看 到 什 麼 了 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 不 的 不 不 的 不 不 不 不 的 的 的 的 不 不 的 的 的 \n",
      "e : 0 (941/3334) loss : 7.159451007843018\n",
      "e : 0 (942/3334) loss : 7.139433860778809\n",
      "e : 0 (943/3334) loss : 6.871147632598877\n",
      "e : 0 (944/3334) loss : 7.031122207641602\n",
      "e : 0 (945/3334) loss : 6.8758544921875\n",
      "e : 0 (946/3334) loss : 7.148237228393555\n",
      "e : 0 (947/3334) loss : 6.834941387176514\n",
      "e : 0 (948/3334) loss : 6.935449123382568\n",
      "e : 0 (949/3334) loss : 6.9231390953063965\n",
      "e : 0 (950/3334) loss : 7.078168869018555\n",
      "input: 有 北 投 公 園 的 八 卦 ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 說 不 定 打 開 來 是 正 妹 喔 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 的 的 不 不 不 不 不 不 不 不 的 的 不 的 不 不 不 的 的 不 不 不 \n",
      "e : 0 (951/3334) loss : 6.909406661987305\n",
      "e : 0 (952/3334) loss : 6.723903656005859\n",
      "e : 0 (953/3334) loss : 6.947503566741943\n",
      "e : 0 (954/3334) loss : 6.929889678955078\n",
      "e : 0 (955/3334) loss : 7.050807952880859\n",
      "e : 0 (956/3334) loss : 7.249621868133545\n",
      "e : 0 (957/3334) loss : 7.255660057067871\n",
      "e : 0 (958/3334) loss : 6.898562908172607\n",
      "e : 0 (959/3334) loss : 6.745982646942139\n",
      "e : 0 (960/3334) loss : 6.84813928604126\n",
      "input: 新 北 市 很 擁 擠 的 原 因 是 什 麼 ？ [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 天 龍 人 愛 擠 一 起 生 活 啊 怪 誰 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 不 的 的 的 的 的 不 一 不 的 的 的 的 的 不 的 的 不 不 的 不 \n",
      "e : 0 (961/3334) loss : 6.99047327041626\n",
      "e : 0 (962/3334) loss : 6.898153781890869\n",
      "e : 0 (963/3334) loss : 6.7118072509765625\n",
      "e : 0 (964/3334) loss : 7.569454669952393\n",
      "e : 0 (965/3334) loss : 6.882472038269043\n",
      "e : 0 (966/3334) loss : 6.673220157623291\n",
      "e : 0 (967/3334) loss : 7.324352264404297\n",
      "e : 0 (968/3334) loss : 6.680675983428955\n",
      "e : 0 (969/3334) loss : 6.884792804718018\n",
      "e : 0 (970/3334) loss : 6.777181148529053\n",
      "input: 台 灣 的 公 媽 廳 是 哪 個 天 才 發 明 的 ? [PAD] [PAD] [PAD] \n",
      "target: 南 部 人 透 天 空 間 大 啊 ！ 北 部 人 鳥 籠 子 住 都 嫌 不 夠 了 。 。 \n",
      "chatbot: 就 不 不 的 不 的 的 的 的 不 不 的 不 不 的 的 不 的 不 的 的 的 不 不 \n",
      "e : 0 (971/3334) loss : 6.557408809661865\n",
      "e : 0 (972/3334) loss : 7.048332214355469\n",
      "e : 0 (973/3334) loss : 6.812860488891602\n",
      "e : 0 (974/3334) loss : 6.798993110656738\n",
      "e : 0 (975/3334) loss : 7.000016689300537\n",
      "e : 0 (976/3334) loss : 6.951443672180176\n",
      "e : 0 (977/3334) loss : 6.612495422363281\n",
      "e : 0 (978/3334) loss : 7.129889488220215\n",
      "e : 0 (979/3334) loss : 6.858413219451904\n",
      "e : 0 (980/3334) loss : 6.664350986480713\n",
      "input: 有 沒 有 [UNK] 當 履 歷 表 在 寫 的 八 卦 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 我 啊 ! 自 宅 守 備 員 嗆 我 嗆 夠 沒 ~ ~ 。 [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 的 不 的 不 的 不 不 的 不 不 不 的 的 的 的 的 的 \n",
      "e : 0 (981/3334) loss : 6.897384166717529\n",
      "e : 0 (982/3334) loss : 7.212587356567383\n",
      "e : 0 (983/3334) loss : 6.699553489685059\n",
      "e : 0 (984/3334) loss : 6.773873805999756\n",
      "e : 0 (985/3334) loss : 6.73617696762085\n",
      "e : 0 (986/3334) loss : 7.274165153503418\n",
      "e : 0 (987/3334) loss : 7.101942539215088\n",
      "e : 0 (988/3334) loss : 7.170258522033691\n",
      "e : 0 (989/3334) loss : 6.887450695037842\n",
      "e : 0 (990/3334) loss : 6.890424728393555\n",
      "input: 台 中 的 幹 嘛 多 設 置 一 個 百 貨 集 中 區 阿 [PAD] [PAD] \n",
      "target: 正 名 台 灣 大 道 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 不 不 的 的 不 不 的 不 不 不 不 的 的 不 不 不 不 的 不 \n",
      "e : 0 (991/3334) loss : 6.946077346801758\n",
      "e : 0 (992/3334) loss : 6.974584579467773\n",
      "e : 0 (993/3334) loss : 6.984045028686523\n",
      "e : 0 (994/3334) loss : 7.089486122131348\n",
      "e : 0 (995/3334) loss : 6.652506351470947\n",
      "e : 0 (996/3334) loss : 6.852346897125244\n",
      "e : 0 (997/3334) loss : 7.086753845214844\n",
      "e : 0 (998/3334) loss : 6.781862735748291\n",
      "e : 0 (999/3334) loss : 6.670207977294922\n",
      "e : 0 (1000/3334) loss : 6.743429660797119\n",
      "input: 噓 寒 問 暖 女 生 到 底 信 不 信 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 人 帥 真 溫 暖 人 醜 有 夠 煩 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 的 不 不 的 的 的 的 不 的 不 的 不 不 的 不 的 的 \n",
      "e : 0 (1001/3334) loss : 6.86259651184082\n",
      "e : 0 (1002/3334) loss : 6.7184343338012695\n",
      "e : 0 (1003/3334) loss : 7.14515495300293\n",
      "e : 0 (1004/3334) loss : 6.8763251304626465\n",
      "e : 0 (1005/3334) loss : 6.991924285888672\n",
      "e : 0 (1006/3334) loss : 6.925708770751953\n",
      "e : 0 (1007/3334) loss : 6.640379428863525\n",
      "e : 0 (1008/3334) loss : 7.228320598602295\n",
      "e : 0 (1009/3334) loss : 6.9779744148254395\n",
      "e : 0 (1010/3334) loss : 7.07772159576416\n",
      "input: 有 沒 有 郭 美 珠 很 久 沒 爆 卦 的 卦 ？ [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 郭 美 珠 楊 麗 花 為 了 生 活 來 唸 歌 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 不 不 不 的 不 的 不 不 不 不 的 不 不 的 不 的 的 不 的 的 的 的 \n",
      "e : 0 (1011/3334) loss : 6.873241901397705\n",
      "e : 0 (1012/3334) loss : 7.031844139099121\n",
      "e : 0 (1013/3334) loss : 6.9128193855285645\n",
      "e : 0 (1014/3334) loss : 7.169998645782471\n",
      "e : 0 (1015/3334) loss : 6.8616108894348145\n",
      "e : 0 (1016/3334) loss : 6.870593547821045\n",
      "e : 0 (1017/3334) loss : 7.306796550750732\n",
      "e : 0 (1018/3334) loss : 6.820443153381348\n",
      "e : 0 (1019/3334) loss : 7.301218509674072\n",
      "e : 0 (1020/3334) loss : 7.199112415313721\n",
      "input: 東 京 通 勤 2 個 半 小 時 的 人 怎 麼 生 活 的 ? \n",
      "target: 八 德 到 新 莊 兩 條 路 ， 一 是 從 鶯 歌 往 樹 林 再 到 新 樹 路 ； 。 [PAD] \n",
      "chatbot: 的 的 的 不 的 不 不 不 不 不 不 不 的 的 的 的 不 的 不 的 的 不 不 的 就 \n",
      "e : 0 (1021/3334) loss : 6.8549933433532715\n",
      "e : 0 (1022/3334) loss : 6.911717414855957\n",
      "e : 0 (1023/3334) loss : 6.925454139709473\n",
      "e : 0 (1024/3334) loss : 6.984080791473389\n",
      "e : 0 (1025/3334) loss : 6.933999538421631\n",
      "e : 0 (1026/3334) loss : 7.256359577178955\n",
      "e : 0 (1027/3334) loss : 7.140782833099365\n",
      "e : 0 (1028/3334) loss : 7.104954719543457\n",
      "e : 0 (1029/3334) loss : 6.866290092468262\n",
      "e : 0 (1030/3334) loss : 6.940166473388672\n",
      "input: 惡 魔 是 不 是 很 愛 插 耶 穌 甘 蔗 啊 [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 我 們 這 叫 種 芭 樂 ( 台 語 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 不 的 的 的 不 不 不 的 不 不 不 不 不 不 不 不 不 的 不 不 \n",
      "e : 0 (1031/3334) loss : 6.8103108406066895\n",
      "e : 0 (1032/3334) loss : 6.872711658477783\n",
      "e : 0 (1033/3334) loss : 6.993144512176514\n",
      "e : 0 (1034/3334) loss : 6.982908248901367\n",
      "e : 0 (1035/3334) loss : 6.8289570808410645\n",
      "e : 0 (1036/3334) loss : 6.933177947998047\n",
      "e : 0 (1037/3334) loss : 7.180617332458496\n",
      "e : 0 (1038/3334) loss : 6.949359893798828\n",
      "e : 0 (1039/3334) loss : 7.12067985534668\n",
      "e : 0 (1040/3334) loss : 6.905040740966797\n",
      "input: [UNK] . . . 內 ㄍ . . . 有 人 被 警 察 逮 捕 過 嗎 ？ \n",
      "target: 關 到 死 好 了 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 不 一 一 的 的 的 不 不 不 不 的 的 就 的 不 的 的 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (1041/3334) loss : 7.334344863891602\n",
      "e : 0 (1042/3334) loss : 7.058819770812988\n",
      "e : 0 (1043/3334) loss : 6.862817287445068\n",
      "e : 0 (1044/3334) loss : 6.659632205963135\n",
      "e : 0 (1045/3334) loss : 7.072122573852539\n",
      "e : 0 (1046/3334) loss : 7.237457752227783\n",
      "e : 0 (1047/3334) loss : 7.016625881195068\n",
      "e : 0 (1048/3334) loss : 6.865620136260986\n",
      "e : 0 (1049/3334) loss : 7.298626899719238\n",
      "e : 0 (1050/3334) loss : 7.087352275848389\n",
      "input: 畢 業 典 禮 沒 收 到 花 怎 麼 辦 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 國 小 畢 業 ？ 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 不 的 的 的 的 不 的 的 不 的 的 的 的 不 的 \n",
      "e : 0 (1051/3334) loss : 6.937058448791504\n",
      "e : 0 (1052/3334) loss : 7.047806262969971\n",
      "e : 0 (1053/3334) loss : 6.830868244171143\n",
      "e : 0 (1054/3334) loss : 6.928366184234619\n",
      "e : 0 (1055/3334) loss : 6.941479206085205\n",
      "e : 0 (1056/3334) loss : 6.994722843170166\n",
      "e : 0 (1057/3334) loss : 7.179230213165283\n",
      "e : 0 (1058/3334) loss : 6.846347808837891\n",
      "e : 0 (1059/3334) loss : 6.755615234375\n",
      "e : 0 (1060/3334) loss : 6.803353786468506\n",
      "input: 大 4 學 生 一 個 人 兼 3 份 工 作 可 以 嗎 ? [PAD] [PAD] \n",
      "target: 賣 屁 股 一 份 就 夠 惹 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 不 的 的 不 不 的 的 不 不 不 不 不 的 不 不 的 不 不 的 不 \n",
      "e : 0 (1061/3334) loss : 7.015438556671143\n",
      "e : 0 (1062/3334) loss : 7.298361778259277\n",
      "e : 0 (1063/3334) loss : 7.1201701164245605\n",
      "e : 0 (1064/3334) loss : 7.027987480163574\n",
      "e : 0 (1065/3334) loss : 6.7298078536987305\n",
      "e : 0 (1066/3334) loss : 6.933759689331055\n",
      "e : 0 (1067/3334) loss : 7.023048400878906\n",
      "e : 0 (1068/3334) loss : 6.9478583335876465\n",
      "e : 0 (1069/3334) loss : 7.307077884674072\n",
      "e : 0 (1070/3334) loss : 6.931572437286377\n",
      "input: 大 家 在 八 卦 版 上 學 到 了 什 麼 [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 我 學 會 如 何 頂 爆 別 人 老 婆 和 妹 妹 的 肺 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 不 的 不 的 的 不 的 不 不 不 不 的 一 不 的 不 不 的 不 的 不 \n",
      "e : 0 (1071/3334) loss : 7.081066131591797\n",
      "e : 0 (1072/3334) loss : 6.892953872680664\n",
      "e : 0 (1073/3334) loss : 7.072940349578857\n",
      "e : 0 (1074/3334) loss : 7.011102199554443\n",
      "e : 0 (1075/3334) loss : 6.937216758728027\n",
      "e : 0 (1076/3334) loss : 6.763830661773682\n",
      "e : 0 (1077/3334) loss : 7.003490447998047\n",
      "e : 0 (1078/3334) loss : 7.031978130340576\n",
      "e : 0 (1079/3334) loss : 7.048978328704834\n",
      "e : 0 (1080/3334) loss : 6.859093189239502\n",
      "input: 如 果 基 本 工 資 提 高 到 40 ##k [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 你 法 國 調 成 類 似 台 灣 22 ##k , 他 們 失 業 率 會 大 減 嗎 ？ 。 [PAD] [PAD] \n",
      "chatbot: 的 的 的 的 的 的 不 的 不 的 不 的 不 不 不 的 的 不 不 的 的 的 的 的 一 \n",
      "e : 0 (1081/3334) loss : 6.86975622177124\n",
      "e : 0 (1082/3334) loss : 7.017614364624023\n",
      "e : 0 (1083/3334) loss : 7.0203070640563965\n",
      "e : 0 (1084/3334) loss : 6.75289249420166\n",
      "e : 0 (1085/3334) loss : 6.663478851318359\n",
      "e : 0 (1086/3334) loss : 7.072378158569336\n",
      "e : 0 (1087/3334) loss : 6.899302959442139\n",
      "e : 0 (1088/3334) loss : 6.699746608734131\n",
      "e : 0 (1089/3334) loss : 6.77549409866333\n",
      "e : 0 (1090/3334) loss : 6.8709187507629395\n",
      "input: 有 沒 有 低 素 質 老 外 的 八 卦 ？ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 台 灣 人 更 多 好 嗎 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 的 的 不 不 的 不 不 的 不 的 不 不 的 的 的 不 的 不 的 的 不 的 不 \n",
      "e : 0 (1091/3334) loss : 7.064028739929199\n",
      "e : 0 (1092/3334) loss : 6.676064491271973\n",
      "e : 0 (1093/3334) loss : 7.013394832611084\n",
      "e : 0 (1094/3334) loss : 6.942028522491455\n",
      "e : 0 (1095/3334) loss : 6.9856414794921875\n",
      "e : 0 (1096/3334) loss : 6.80430269241333\n",
      "e : 0 (1097/3334) loss : 6.7449541091918945\n",
      "e : 0 (1098/3334) loss : 6.733058452606201\n",
      "e : 0 (1099/3334) loss : 7.668424606323242\n",
      "e : 0 (1100/3334) loss : 6.990130424499512\n",
      "input: 如 何 攻 略 高 嶺 之 花 ? ? ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 先 拆 坐 墊 開 八 萬 一 ， 之 後 就 常 在 法 院 見 面 。 [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 不 就 的 的 的 不 的 不 的 的 不 的 的 的 的 不 不 的 不 不 不 不 的 \n",
      "e : 0 (1101/3334) loss : 6.7290120124816895\n",
      "e : 0 (1102/3334) loss : 7.022407531738281\n",
      "e : 0 (1103/3334) loss : 6.7244744300842285\n",
      "e : 0 (1104/3334) loss : 6.9601149559021\n",
      "e : 0 (1105/3334) loss : 7.046555519104004\n",
      "e : 0 (1106/3334) loss : 6.893868446350098\n",
      "e : 0 (1107/3334) loss : 6.873767852783203\n",
      "e : 0 (1108/3334) loss : 6.802164077758789\n",
      "e : 0 (1109/3334) loss : 7.019430160522461\n",
      "e : 0 (1110/3334) loss : 7.14369535446167\n",
      "input: 烤 肉 帶 什 麼 食 材 才 內 行 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 沙 耶 ： 人 來 就 可 以 了 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 的 的 不 的 不 的 的 的 的 不 的 不 的 不 不 不 一 不 的 的 不 的 欸 \n",
      "e : 0 (1111/3334) loss : 6.944559097290039\n",
      "e : 0 (1112/3334) loss : 6.5730133056640625\n",
      "e : 0 (1113/3334) loss : 7.117791175842285\n",
      "e : 0 (1114/3334) loss : 6.954141616821289\n",
      "e : 0 (1115/3334) loss : 7.207530975341797\n",
      "e : 0 (1116/3334) loss : 6.797905921936035\n",
      "e : 0 (1117/3334) loss : 6.574234962463379\n",
      "e : 0 (1118/3334) loss : 6.7891340255737305\n",
      "e : 0 (1119/3334) loss : 6.979361534118652\n",
      "e : 0 (1120/3334) loss : 7.081450462341309\n",
      "input: 有 沒 有 冰 球 是 最 狂 運 動 的 八 卦 [PAD] [PAD] [PAD] [PAD] \n",
      "target: 之 前 還 有 割 喉 的 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 不 不 的 不 的 不 不 不 的 不 不 不 不 不 不 不 的 的 \n",
      "e : 0 (1121/3334) loss : 6.768329620361328\n",
      "e : 0 (1122/3334) loss : 7.034851551055908\n",
      "e : 0 (1123/3334) loss : 7.35160493850708\n",
      "e : 0 (1124/3334) loss : 6.998970985412598\n",
      "e : 0 (1125/3334) loss : 6.729953289031982\n",
      "e : 0 (1126/3334) loss : 6.986286163330078\n",
      "e : 0 (1127/3334) loss : 6.730739116668701\n",
      "e : 0 (1128/3334) loss : 7.139938831329346\n",
      "e : 0 (1129/3334) loss : 6.977629661560059\n",
      "e : 0 (1130/3334) loss : 7.248518943786621\n",
      "input: 小 玉 有 沒 繳 稅 ? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 下 一 個 影 片 題 材 不 要 破 梗 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 的 不 不 不 的 的 不 不 不 的 的 的 就 的 的 的 的 不 不 的 的 \n",
      "e : 0 (1131/3334) loss : 7.196559906005859\n",
      "e : 0 (1132/3334) loss : 7.045564651489258\n",
      "e : 0 (1133/3334) loss : 7.107300281524658\n",
      "e : 0 (1134/3334) loss : 6.8488898277282715\n",
      "e : 0 (1135/3334) loss : 6.970541954040527\n",
      "e : 0 (1136/3334) loss : 7.166345596313477\n",
      "e : 0 (1137/3334) loss : 6.940170764923096\n",
      "e : 0 (1138/3334) loss : 7.162539958953857\n",
      "e : 0 (1139/3334) loss : 6.950395107269287\n",
      "e : 0 (1140/3334) loss : 7.000784397125244\n",
      "input: 同 志 結 婚 跟 領 養 扯 再 一 起 的 八 卦 [PAD] [PAD] [PAD] [PAD] \n",
      "target: 說 真 的 領 養 這 個 議 題 可 能 反 對 的 人 比 較 多 。 \n",
      "chatbot: 不 不 不 的 不 不 不 不 不 的 的 不 的 的 的 的 不 不 的 \n",
      "e : 0 (1141/3334) loss : 7.245989799499512\n",
      "e : 0 (1142/3334) loss : 6.79264497756958\n",
      "e : 0 (1143/3334) loss : 6.917013168334961\n",
      "e : 0 (1144/3334) loss : 6.8514885902404785\n",
      "e : 0 (1145/3334) loss : 6.778852939605713\n",
      "e : 0 (1146/3334) loss : 6.972477436065674\n",
      "e : 0 (1147/3334) loss : 6.882796287536621\n",
      "e : 0 (1148/3334) loss : 6.931425094604492\n",
      "e : 0 (1149/3334) loss : 6.840421676635742\n",
      "e : 0 (1150/3334) loss : 6.7223639488220215\n",
      "input: 躺 著 玩 坐 著 玩 還 是 _ _ 好 玩 有 沒 有 掛 ? [PAD] \n",
      "target: 你 簽 名 檔 比 文 還 長 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 的 不 的 的 的 的 不 的 不 的 的 不 的 不 的 不 不 \n",
      "e : 0 (1151/3334) loss : 6.812938690185547\n",
      "e : 0 (1152/3334) loss : 6.9359211921691895\n",
      "e : 0 (1153/3334) loss : 6.85158634185791\n",
      "e : 0 (1154/3334) loss : 6.437439918518066\n",
      "e : 0 (1155/3334) loss : 6.942773342132568\n",
      "e : 0 (1156/3334) loss : 6.753661155700684\n",
      "e : 0 (1157/3334) loss : 6.795210361480713\n",
      "e : 0 (1158/3334) loss : 7.2705912590026855\n",
      "e : 0 (1159/3334) loss : 7.174391746520996\n",
      "e : 0 (1160/3334) loss : 6.965701103210449\n",
      "input: 肥 宅 打 手 槍 會 順 便 練 習 用 保 險 套 嗎 ？ [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "target: 煩 請 女 伴 用 嘴 套 上 . . . . 不 用 自 己 戴 y 。 [PAD] [PAD] \n",
      "chatbot: 的 不 不 的 的 的 不 不 就 不 的 不 的 不 不 的 的 的 的 不 一 \n",
      "e : 0 (1161/3334) loss : 6.658569812774658\n",
      "e : 0 (1162/3334) loss : 7.146296977996826\n",
      "e : 0 (1163/3334) loss : 7.18590784072876\n",
      "e : 0 (1164/3334) loss : 6.871240615844727\n",
      "e : 0 (1165/3334) loss : 6.894930839538574\n",
      "e : 0 (1166/3334) loss : 6.916193008422852\n",
      "e : 0 (1167/3334) loss : 6.8290557861328125\n",
      "e : 0 (1168/3334) loss : 6.998752593994141\n",
      "e : 0 (1169/3334) loss : 7.025084018707275\n",
      "e : 0 (1170/3334) loss : 6.775850772857666\n",
      "input: 有 沒 有 仙 境 / [UNK] 半 夜 狂 抓 掛 的 八 卦 [PAD] [PAD] [PAD] \n",
      "target: 然 後 就 發 現 人 都 不 見 了 全 被 抓 光 了 [UNK] 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "chatbot: 的 的 不 的 的 不 的 不 不 不 的 不 的 的 的 的 的 不 的 不 不 的 不 的 的 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e : 0 (1171/3334) loss : 6.828174114227295\n",
      "e : 0 (1172/3334) loss : 6.598046779632568\n",
      "e : 0 (1173/3334) loss : 7.10786247253418\n",
      "e : 0 (1174/3334) loss : 6.7446746826171875\n",
      "e : 0 (1175/3334) loss : 7.232537269592285\n",
      "e : 0 (1176/3334) loss : 7.225384712219238\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "243462",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-b993900c82b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-5e3797c1bb97>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#         A_token = tokenize_and_cut(\"[CLS] \" + str(self.train_A[index]) + \" [SEP]\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mQ_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_and_cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mA_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_and_cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_A\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"。\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4403\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4404\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4405\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4406\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4407\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 243462"
     ]
    }
   ],
   "source": [
    "TRAIN_DIALOG = True\n",
    "for e in range(1): #lucky number\n",
    "    \n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i, t in train_data:\n",
    "        \n",
    "        try:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            i= i.view(i.shape[0],i.shape[1])\n",
    "            t = t.view(t.shape[0],t.shape[1])\n",
    "            \n",
    "            # src = [src len, batch size]\n",
    "            # tgt = [output_len, batch size]\n",
    "\n",
    "            output =  g_model(i, t, 0.5)\n",
    "            output_dim = output.shape[-1]\n",
    "            loss = loss_fn(output.view(-1, output_dim), t.view(-1))\n",
    "            \n",
    "#             print(output.size())\n",
    "#             print(output_dim)\n",
    "#             print(output.view(-1, output_dim).size())\n",
    "#             print(t.view(-1).size())\n",
    "\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            count += 1\n",
    "            mean_loss+=loss\n",
    "     \n",
    "        except RuntimeError: \n",
    "            print('error.....1')\n",
    "        except AttributeError: \n",
    "            print('error.....2')\n",
    "        else:\n",
    "            print(\"e : \" + str(e) + ' (' + str(count) + '/' + str(iter) + ') loss : ' + str(float(loss)) )\n",
    "            if count % 10 == 0:\n",
    "                print_input_tensor(i)\n",
    "                print_target_tensor(t)\n",
    "                print_chat(output)\n",
    "\n",
    "    print(\"*****e : \"+str(e)+\" L : \"+str(mean_loss/count)+\"*****\")\n",
    "        \n",
    "    torch.save(g_model, \"./model_\"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TransBertEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TransBertDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type GruEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type GruDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type DialogDNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# torch.save(g_model, \"./model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DataLoader(v_set, shuffle=True, batch_size=1, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chatbot.png'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:\\Program Files\\Graphviz 2.44.1\\bin'\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\graphviz'\n",
    "TRAIN_DIALOG = True\n",
    "x,y = train_set.__getitem__(0)\n",
    "make_dot(g_model(x,y,1), params=dict(g_model.named_parameters())).render(\"chatbot\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
